{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Three \n",
    "\n",
    "The primary description of this coursework is available on the CM20252 Moodle page. This is the Jupyter notebook you must complete and submit to receive marks. This notebook adds additional detail to the coursework specification but does not repeat the information that has already been provided there. \n",
    "\n",
    "You must follow all instructions given in this notebook precisely.\n",
    "\n",
    "Restart the kernel and run all cells before submitting the notebook. This will guarantee that we will be able to run your code for testing. Remember to save your work regularly.\n",
    "\n",
    "__You will develop players for Connect-Three on a grid that is 5 columns wide and 3 rows high. An example is shown below showing a win for Player Red.__\n",
    "\n",
    "<img src=\"images/connect3.png\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "For your reference, below is a visual depiction of the agent-environment interface in reinforcment learning. The interaction of the agent with its environments starts at decision stage $t=0$ with the observation of the current state $s_0$. (Notice that there is no reward at this initial stage.) The agent then chooses an action to execute at decision stage $t=1$. The environment responds by changing its state to $s_1$ and returning the numerical reward signal $r_1$. \n",
    "\n",
    "<img src=\"images/agent-environment.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "Below, we provide some code that will be useful for implementing parts of this interface. You are not obligated to use this code; please feel free to develop your own code from scratch. \n",
    "\n",
    "### Code details\n",
    "\n",
    "We provide a `Connect` class that you can use to simulate Connect-Three games. The following cells in this section will walk you through the basic usage of this class by playing a couple of games.\n",
    "\n",
    "We import the `connect` module and create a Connect-Three environmnet called `env`. The constructor method has one argument called `verbose`. If `verbose=True`, the `Connect` object will regularly print the progress of the game. This is useful for getting to know the provided code, debugging your code, or if you just want to play around. You will want to set `verbose=False` when you run hundreds of episodes to complete the marked exercises.\n",
    "\n",
    "This `Connect` environment uses the strings `'o'` and `'x'` instead of different disk colors in order to distuingish between the two players.\n",
    "\n",
    "Before we start a game, we should call the `reset()` method. This method cleans the board and resets other state variables. The `first_player` argument can be specified (`'o'` or `'x'`) to deterministically choose the starting player. It defaults to `\"random\"`, in which case each player starts the game with probability of $\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import connect\n",
    "env = connect.Connect(verbose=True)\n",
    "env.reset(first_player='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interact with the environment using the `act()` method. This method takes an `action` as input and computes the response of the environment. An action is defined as the column index that a disk is dropped into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `act()` method inserts a disk into the specified column. \n",
    "\n",
    "If we want to change the player on move, we can do so by using the `change_turn()` method. We can check whose turn it is by accessing the `.player_at_turn` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Current player at turn:\", env.player_at_turn)\n",
    "env.change_turn()\n",
    "print(\"Current player at turn:\", env.player_at_turn)\n",
    "\n",
    "# Drop another disk into the same centre column.\n",
    "env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we set `verbose=True`, the grid is printed each time we call the `act()` method. This grid is stored as a two-dimensional numpy array in the connect class and you can easily access by calling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_grid = env.grid\n",
    "print(current_grid)\n",
    "# Notice that the grid now appears to be \"upside down\" because numpy arrays are printed from \"top to bottom\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's make another move.\n",
    "env.change_turn()\n",
    "env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we make another move with `act(action=2)`, the environment will throw an error because that column is already filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell should throw an error!\n",
    "env.change_turn()\n",
    "env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `.available_actions` contains a numpy array of all not yet filled columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(env.available_actions)\n",
    "# Column index '2' is missing because this column is already filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Connect` class contains a method called `was_winning_move()` that checks whether the last move won the game (returns `True`) or not (returns `False`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obviously the game has not yet been won by any player.\n",
    "print(\"Winning move?\", env.was_winning_move()) \n",
    "\n",
    "# Make some moves\n",
    "env.act(action=3)\n",
    "env.change_turn()\n",
    "env.act(action=1)\n",
    "env.change_turn()\n",
    "env.act(action=3)\n",
    "env.change_turn()\n",
    "env.act(action=0)\n",
    "\n",
    "# Check again whether a player has won the game.\n",
    "print(\"Winning move?\", env.was_winning_move()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `Connect` class contains a method called `grid_is_full()` that checks whether the grid still contains empty slots. You can use this method to check whether the game is a draw.\n",
    "\n",
    "Feel free to modify existing or add new methods to the `Connect` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Q-learning\n",
    "\n",
    "Your opponent is always the first player. Your agent is always the second player.\n",
    "\n",
    "For your reference, the pseudo-code for Q-learning is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 1998, Section 6.5).\n",
    "<img src=\"images/q_learning.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "Prepare a **learning curve** following the directions below. We refer to this as Plot 1.\n",
    "\n",
    "After $n$ steps of interaction with the environment, play $m$ games with the current policy of the agent (without modifying the policy). Think of this as interrupting the agent for a period of time to test how well it has learned so far. Your plot should show the total score obtained in these $m$ games as a function of $n, 2n, 3n, … kn$. The choices of $n$ and $k$ are up to you. They should be reasonable values that demonstrate the efficiency of the learning and how well the agent learns to play the game eventually. Use $m=10$. \n",
    "\n",
    "This plot should show the mean performance of $a$ agents, not the performance of a single agent. Because of the stochasticity in the environment, you will obtain two different learning curves from two different agents even though they are using exactly the same algorithm. We suggest setting $a$ to 30 or higher.\n",
    "\n",
    "Present a single mean learning curve with your choice of parameters $\\epsilon$ and $\\alpha$. The plot should also show (as a baseline) the mean performance of a random agent that does not learn but chooses actions uniformly randomly from among the legal actions. Label this line “Random Agent”. \n",
    "\n",
    "Please include this plot as a static figure in the appropriate cell below. You can look at the source code of this markdown cell to find out how to embed figures using html or you can use drag & drop. If you link to locally stored images, make sure to include those in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "1431aa87b9e9019a4dbe6e696e0a9082",
     "grade": true,
     "grade_id": "cell-3ac2114f764e8410",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import connect\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import RandomAgent as ra\n",
    "\n",
    "K=200\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.game=connect.Connect(verbose=False)\n",
    "        self.game.reset(first_player='o')\n",
    "    \n",
    "    def choiceRandAction(self):\n",
    "        available=self.game.available_actions\n",
    "        return available[random.randint(0,len(available.tolist())-1)]\n",
    "    \n",
    "    def makeRandAction(self):\n",
    "        action=self.choiceRandAction()\n",
    "        self.game.act(action)\n",
    "\n",
    "    def makeEnvAction(self,isfirst):\n",
    "        if isfirst:\n",
    "            self.game.reset(first_player='o')\n",
    "            self.makeRandAction()\n",
    "            self.game.change_turn()\n",
    "        else:\n",
    "            if self.game.was_winning_move('x'):\n",
    "                self.game.reset(first_player='o')\n",
    "                self.makeRandAction()\n",
    "                self.game.change_turn()\n",
    "            elif self.game.grid_is_full():#means a draw\n",
    "                self.game.reset(first_player='o')\n",
    "                self.makeRandAction()\n",
    "                self.game.change_turn()\n",
    "            else:\n",
    "                self.makeRandAction()\n",
    "                self.game.change_turn()\n",
    "\n",
    "            \n",
    "class MemoryUnit:\n",
    "    def __init__(self,state):\n",
    "        self.state=state\n",
    "        self.actionValues={0:0,1:0,2:0,3:0,4:0}\n",
    "\n",
    "    def getmaxAction(self,available_actions):\n",
    "        maxvalue=-1\n",
    "        maxaction=-1\n",
    "        for action in available_actions:\n",
    "            if self.actionValues[action]>=maxvalue:\n",
    "                maxvalue=self.actionValues[action]\n",
    "                maxaction=action\n",
    "        return maxaction\n",
    "    \n",
    "    def getmaxActionValue(self):\n",
    "        maxvalue=-1\n",
    "        for action in self.actionValues:\n",
    "            if self.actionValues[action]>=maxvalue:\n",
    "                maxvalue=self.actionValues[action]   \n",
    "        return maxvalue\n",
    "\n",
    "    def setActionValue(self,action,value):#if there is the memrory about a state,can use this method to update the value of its action \n",
    "        self.actionValues[action]=value\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self,environment=Environment()):\n",
    "        self.workmemory={}#previous{'state':xxx,'action':yyy]\n",
    "        self.longtermmemory=[]#list of MemoryUnit\n",
    "        self.environment=environment\n",
    "        self.γ=0.7\n",
    "        self.α=0.7\n",
    "        self.ε=0.05\n",
    "        self.resultlist=[]\n",
    "    def searchMemory(self,state):#return a MemoryUnit\n",
    "        result=False\n",
    "        for unit in self.longtermmemory:\n",
    "            if self.stateEq(state,unit.state):\n",
    "                result=unit\n",
    "                break\n",
    "        return result\n",
    "    \n",
    "    def editMemory(self,MemoryUnit,action,value):\n",
    "        MemoryUnit.setActionValue(action,value)\n",
    "\n",
    "    def addMemory(self,state,action,value):#called in the case that when a state first insert into memory\n",
    "        newMemoryUnit=MemoryUnit(state)\n",
    "        newMemoryUnit.actionValues[action]=value\n",
    "        self.longtermmemory.append(newMemoryUnit)\n",
    "\n",
    "    def stateEq(self,state1,state2):#state is a numpy array\n",
    "        return (state1==state2).all()\n",
    "\n",
    "    def getChoiceAction(self):#get a choose action from a given state\n",
    "        memory=False\n",
    "        memory=self.searchMemory(self.environment.game.grid)\n",
    "        if memory==False:\n",
    "            available_actions=self.environment.game.available_actions.tolist()\n",
    "            return available_actions[random.randint(0,len(available_actions)-1)]\n",
    "        else:\n",
    "            if random.random()>self.ε:\n",
    "                return memory.getmaxAction(self.environment.game.available_actions.tolist())#########\n",
    "            else:\n",
    "                available_actions=self.environment.game.available_actions.tolist()\n",
    "                return available_actions[random.randint(0,len(available_actions)-1)]\n",
    "    \n",
    "    def getNowstate(self):#get the state of the env now\n",
    "        return self.environment.game.grid\n",
    "    \n",
    "    def makeAction(self):\n",
    "        action=self.getChoiceAction()\n",
    "        self.workmemory={'state':copy.deepcopy(self.getNowstate()),'action':copy.deepcopy(action)}#let the workmemory be the state-action pair just have made\n",
    "        self.environment.game.act(action)\n",
    "        self.environment.game.change_turn()\n",
    "\n",
    "    def getReward(self):#only deal with the case that the agent action is not a winning move or draw move(that means the enemy have just made a action)\n",
    "        R=0\n",
    "        memoryReward=0#init two part of reward\n",
    "        '''\n",
    "        if self.environment.game.was_winning_move('o'):\n",
    "            R=-1\n",
    "            memoryReward=0\n",
    "        elif self.environment.game.grid_is_full():\n",
    "            R=0\n",
    "            memoryReward=0\n",
    "        else:\n",
    "        '''\n",
    "        R=0\n",
    "        nowstate=self.getNowstate()\n",
    "        findMemoryUnit=self.searchMemory(nowstate)\n",
    "        if findMemoryUnit==False:\n",
    "            memoryReward=0\n",
    "        else:\n",
    "            memoryReward=findMemoryUnit.getmaxActionValue()\n",
    "        return {'R':R,'memoryreward':memoryReward}\n",
    "\n",
    "\n",
    "    def learn(self,reward):#learn by init and add memoryUnit into memory or edit existed memoryUnit\n",
    "        R=reward['R']\n",
    "        memoreward=reward['memoryreward']\n",
    "        prevoiusState=self.workmemory['state']\n",
    "        prevoiusAction=self.workmemory['action']\n",
    "        findMemoryUnit=self.searchMemory(prevoiusState)\n",
    "        if findMemoryUnit==False:\n",
    "            initvalue=self.α*(R+self.γ*memoreward)\n",
    "            self.addMemory(prevoiusState,prevoiusAction,initvalue)\n",
    "        else:\n",
    "            oldvalue=findMemoryUnit.actionValues[prevoiusAction]\n",
    "            newvalue=(1-self.α)*oldvalue+self.α*(R+self.γ*memoreward)\n",
    "            self.editMemory(findMemoryUnit,prevoiusAction,newvalue)\n",
    "\n",
    "    def interact(self):#a interact include decide a action,make the action,the action of env(enemy),and learn from reward.\n",
    "        if self.environment.game.grid_is_full():#the action that enemy draw\n",
    "            reward={'R':0,'memoryreward':0}\n",
    "            self.learn(reward)\n",
    "            self.environment.game.reset(first_player='o')\n",
    "            self.environment.makeEnvAction(True)\n",
    "\n",
    "        self.makeAction()\n",
    "\n",
    "        if self.environment.game.was_winning_move('x'):\n",
    "            reward={'R':1,'memoryreward':0}\n",
    "            self.learn(reward)\n",
    "            self.environment.makeEnvAction(True)\n",
    "        elif self.environment.game.grid_is_full():#means a draw\n",
    "            reward={'R':0,'memoryreward':0}\n",
    "            self.learn(reward)\n",
    "            self.environment.makeEnvAction(True)\n",
    "        else:\n",
    "            self.environment.makeEnvAction(False)\n",
    "            if self.environment.game.grid_is_full():#the action that enemy draw\n",
    "                reward={'R':0,'memoryreward':0}\n",
    "                self.learn(reward)\n",
    "                self.environment.game.reset(first_player='o')\n",
    "                self.environment.makeEnvAction(True)\n",
    "            elif self.environment.game.was_winning_move('o'):\n",
    "                reward={'R':-1,'memoryreward':0}\n",
    "                self.learn(reward)\n",
    "                self.environment.game.reset(first_player='o')\n",
    "                self.environment.makeEnvAction(True)\n",
    "            else:\n",
    "                reward=self.getReward()\n",
    "                self.learn(reward)\n",
    "\n",
    "    def play_a_game(self):#return the result of the game(win:1,lose:-1,draw:0)\n",
    "        def judgeGameState():\n",
    "            if self.environment.game.player_at_turn=='o' and self.environment.game.was_winning_move('x'):\n",
    "                return win#player_at_turn is after a change made by previousActionner,so'o'means previous action is made my agent\n",
    "            elif self.environment.game.player_at_turn=='x' and  self.environment.game.was_winning_move('o'):\n",
    "                return lose\n",
    "            elif self.environment.game.grid_is_full():\n",
    "                return draw\n",
    "            else:\n",
    "                return running\n",
    "\n",
    "        self.environment.game.reset(first_player='o')\n",
    "        running=10\n",
    "        win=1\n",
    "        lose=-1\n",
    "        draw=0#four kinds of game states\n",
    "        step=0\n",
    "        while(True):\n",
    "            if step==0:\n",
    "                isfirst=True\n",
    "            else:\n",
    "                isfirst=False\n",
    "            self.environment.makeEnvAction(isfirst)\n",
    "            newgamestate=judgeGameState()\n",
    "\n",
    "            if newgamestate==win:\n",
    "                return win\n",
    "            elif newgamestate==lose:\n",
    "                return lose\n",
    "            elif newgamestate==draw:\n",
    "                return draw\n",
    "\n",
    "            self.makeAction()\n",
    "            newgamestate=judgeGameState()\n",
    "  \n",
    "            if newgamestate==win:\n",
    "                return win\n",
    "            elif newgamestate==lose:\n",
    "                return lose\n",
    "            elif newgamestate==draw:\n",
    "                return draw\n",
    "            step+=1\n",
    "        \n",
    "    def trainAndShow(self,k=K,n=20,m=10):\n",
    "        def play_m_games_and_sum_up():\n",
    "            summary=0\n",
    "            newAgent=Agent()\n",
    "            newAgent.longtermmemory=copy.deepcopy(self.longtermmemory)\n",
    "            for i in range(m):\n",
    "                summary=summary+newAgent.play_a_game()\n",
    "            return summary\n",
    "\n",
    "        self.environment.game.reset(first_player='o')\n",
    "        self.environment.makeEnvAction(True)\n",
    "        for i in range(k):\n",
    "            for j in range(n):\n",
    "                self.interact()\n",
    "            res=play_m_games_and_sum_up()\n",
    " #           print(res)\n",
    "            self.resultlist.append(res)\n",
    "\n",
    "\n",
    "def draw(resultlist,randlist):\n",
    "    x=list(range(len(resultlist))) \n",
    "    plt.figure()  \n",
    "    plt.plot(x,resultlist,'r', label='Q-learning')\n",
    "    plt.plot(x,randlist,'b',label='Random Agent')\n",
    "    plt.xlabel(\"k\")  \n",
    "    plt.ylabel(\"value\")  \n",
    "    plt.savefig(\"return.jpg\")  \n",
    " \n",
    "agentnum=30\n",
    "\n",
    "agentlsit=[]\n",
    "randomagentlist=[]\n",
    "\n",
    "for i in range(agentnum):\n",
    "    agent=Agent()#####\n",
    "    agentlsit.append(agent)\n",
    "for agent in agentlsit:\n",
    "    agent.trainAndShow()\n",
    "\n",
    "for i in range(agentnum):\n",
    "    agent=ra.randomAgent()#####\n",
    "    randomagentlist.append(agent)\n",
    "for agent in randomagentlist:\n",
    "    agent.trainAndShow()\n",
    "\n",
    "def sumagentlistresult():\n",
    "    retlist=[]\n",
    "    for i in range(K):\n",
    "        retlist.append(0)\n",
    "    for agent in agentlsit:\n",
    "        retlist = map(lambda x, y: x + y, retlist,agent.resultlist)\n",
    "    retlist=list(map(lambda x:x/agentnum,retlist))\n",
    "    return retlist\n",
    "\n",
    "def sumrandomagentlistresult():\n",
    "    retlist=[]\n",
    "    for i in range(K):\n",
    "        retlist.append(0)\n",
    "    for agent in randomagentlist:\n",
    "        retlist = map(lambda x, y: x + y, retlist,agent.resultlist)\n",
    "    retlist=list(map(lambda x:x/agentnum,retlist))\n",
    "    return retlist\n",
    "\n",
    "retlist=sumagentlistresult()\n",
    "randomlist=sumrandomagentlistresult()\n",
    "draw(retlist,randomlist)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "34a84d16a71c19e759cb0afc7b41bbbc",
     "grade": true,
     "grade_id": "cell-ce1405b859519f91",
     "locked": false,
     "points": 30,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "(A) [continued} Insert your static learning curve here (Plot 1).\n",
    "<img src=\"images/part1image.jpg\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "(B) In 3 sentences or less, explain your conclusions from the plot above. How close does your (average) agent get to the best possible level of performance? How efficiently does your (average) agent learn? \n",
    "\n",
    "I select the section(0-200) in witch the agent learn most fast. In turn 400-500 the agent will get to the best possible level of performance,about score 6.5。it is more efficent than i think.\n",
    "\n",
    "(C) In five sentences or less, explain the key aspects of your implementation. How many state-action pairs do you represent in your Q-table? Describe and justify your settings of $\\alpha$ and $\\epsilon$. Are there any things you tried out that are not in your final implementation?\n",
    "\n",
    "In the beginning of the training,there is nothing in the longtern memory of the agent while it will add state-action pair when experience it.and if we look up the memory of a state that not exist yet,we will just get a random choice.so we don't need to search every available state in the memory.I have tried to code the state to a unique number so that i can store it in memory using more efficient data structure than list. \n",
    "\n",
    "(D) In the cell below, make it possible for us to produce from scratch a learning curve similar to Plot 1 but for a single agent, for a $k$ value of your own choosing. You do not need to include the baseline for random play.  This code should run in less than 30 seconds (ours runs in 2 seconds). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e65915a61d304027e4fbd2e714c4beba",
     "grade": true,
     "grade_id": "cell-e0e01e05236aee45",
     "locked": false,
     "points": 40,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzsfXmYJEWZ/htVmVXVV/XAMMM1o7iI\nurIrrouo+HPvS9d1cd11xWtXXfFCYT1WXUAUT8RbREQQPGDBi0vBW1EOQWBRF2EQ5JgZYGYYZqp6\nuqqy8ojfHxFfRmRkZFZWd1V3z5Df8/TTVVmZkZGZkfHG+71ffME45yittNJKK620xVpluStQWmml\nlVbanmEloJRWWmmllTYSKwGltNJKK620kVgJKKWVVlpppY3ESkAprbTSSittJFYCSmmllVZaaSOx\nElBKK6200kobiZWAUlpppZVW2kisBJTSSiuttNJGYs5yV2ApbZ999uEHHXTQclejtNJKK223sptu\nuukhzvmaQfs9ogDloIMOwo033rjc1SittNJK262MMXZvkf1Kl1dppZVWWmkjsRJQSiuttNJKG4mV\ngFJaaaWVVtpIrASU0korrbTSRmIloJRWWmmllTYSKwGltNJKK620kVgJKKWVVlpppY3ESkBZAvvO\nd4B77lnuWiyTtdvA+ecvdy32DAsC4NxzgTBcnvM/+CBwySXjKTsIgC98YfmurbSRWAkoS2BHHw2c\nfvpy12KZ7JJLgJe+FLjvvuWuye5vP/sZ8MpXAtdfvzznP/dc4AUvAHx/9GVfcw3wqleJ/6XttlYC\nyhKY5wH9/nLXYpnM88T/Tmd567EnGN1L+r/U1u0CUSTYxKitbCd7hK1IQGGMPZ4xdov212aMHW/s\n82eMsZa2z7uWq76DLIrE3yPS6MKXqxPck4zu5XI1JmIm4wAUKrNsJ7u1rchcXpzzDQCeDACMsSqA\nzQAutuz6M875c5eybguxMCwBpewoRmCkLyyXzkCd/jjOT2WW7WS3thXJUAz7SwB3cc4LJSdbiRaG\nj2CtsewoRmfLDSglQyltgO0OgPIiAP+T8dszGGO/ZIxdyRg7dCkrVdQ4F3+PeIbS6y1vPfYE25MB\nha6pbCe7ta1oQGGM1QA8D8DXLD/fDODRnPPDAHwagDWekTF2DGPsRsbYjdu2bRtfZTNsufuAZbeS\noYzOlrsxEaCM4/wlQ9kjbEUDCoBnA7iZc77F/IFz3uac75KfrwDgMsb2sex3Fuf8cM754WvWDFwf\nZuRG794jnqGUHcXibbkBhTr90uVVWoatdEA5GhnuLsbYfowxJj8fAXEt25ewboVsuQNzlt1KQBmd\nLXdjGidDKZnsHmErMsoLABhjkwD+GsBrtG2vBQDO+ZkA/hnA6xhjAYAugBdxzvly1DXPlntQuexW\ndhSjs+VuTKUoX9oAW7GAwjnvAFhtbDtT+3w6gBU//7x0eZUMZWS2UgBluRkKpWl55SsBZ8V2YY9I\nW+kur93eSkApAWVkttyAslI0lGuuAV7zGuC660Zfj9IWZSWgjNmWuw9YditdXqOz5W5MSxE2XKSd\nLHcKmtIyrQSUMVvJUMp5KCOzlQIo4wwbLtJO6PzjALbSFmUloIzZlrsPWHYrGcrojMD5ke7yKgFl\nxVoJKGO25Y70XHYrNZTR2XLT3ZUiypeAsmKtBJQx23L3ActuJaCMzpab7q6UsOFxMqXSFmUloIzZ\nlrsPWHYrXV6js+VuTCVDKW2AlYAyZisZSslQRmbLDSilhlLaACsBZcxWAkoJKCOz5QaUkqGUNsBK\nQBmzLXcfsOxWurxGZ8sd5bVSNJQSUFaslYAyZtttGcoNNwA//eniyxnlPJTzzwcefHDx5Yzb+n3g\n9NNH3+EtdnTSagGf/7xYoGchNiyg+D7wmc8U298ceNx+O3D55fn7GuVyDpx1FjA3ZznmgguA++9P\nb/+f/wHe9jbgyiuT23/8Y+Cmm9L733gj8JOfZF6GzXxfNoevXGivw6jsjjuy79kSWQkoY7blHlQu\n2N79bvGiLdZGxVB27QJe+lIBKivdfvIT4I1vHH1qkMWOTi69FDjmGOC++xZ2/LBLAF9zDXDsscC1\n1xYvmwYep54KvPrV+fsagPK734mMLKk+tdMBXvIS4MtfTpf1pjcBH/kIcOKJye1veQvwvvel93/v\ne4Hjjsu/FsOuu040h2tfdoa9DqOyT38aeNWrxld+ASsBZcy22zKUfh/odhdfzqg0FDq+319cOUth\nO3aI/6O4f7otlqHQvVvoPVwIQwGK3QfT5XXffdn1pOun8qURFqUOox9sbZC2GWWh07Gfv98Hdu60\n1yvDqBgf7njbb6+37O9HCShjtt0WUIJgNLrHqACFXpTd4Ua22+L/qHWjxQLKYunysKL8QoR22nfT\npmzgynB5UfVSTSQLNPRt5rk8z37+IFDPt6DFk5tRGW/79f1l15XK3M9jtt1WlA/D0egeo3J5EaDs\nDjeSOpxR5y9bbGNa7OhmWIYyjH6mMxTOgY0bAbF+XtoGAErq9hQBFPOgXs9+nWEoni/n2fXLqG6I\n6njbr+8v+/tRMpQxW8lQRsRQxhmyOmpbqQxlsccPq6EslKHs2CHcZEMylMzqEaCZ5XGeHTHmefbr\nDALRpjud7GsxLMFQxtl+g2DZGUoJKGO2ElBG7PLaHQCl1RL/Rw0oi3VZLRZQhmUowy6aReegoIEF\nurwKMxT9+GFcXoB6xgVsyQBlBbi8SkAZs+3WLq9RdIiPZJfXuBjKQkcni2mMUaTOO6zLaxiGAgB3\n3aWOt4U4Z0R5DQ0o+nfzoCyGQtuG0FGW1OUFLOvotQSUMdtum2141AxlsXpC6fJaXpeX3nmPw+Wl\nl3/nnfnnGhVD0b+b1xeG+QxlCEBZUoYCLCtLWbGAwhi7hzH2a8bYLYyxGy2/M8bYpxhjdzLGfsUY\ne8py1HOQ7dYMJYoW3zhLhjI6W05Ayep882wYhjICQMnUUOj8Zr2zQJL2HxFDWVINBVjWd2SlR3n9\nOef8oYzfng3gEPn3NACflf9XlO3WGgogXi5nEc1E71SGiIxJ2e4UNjwuDWU5w4bz3ENZthBRHlAu\nL0C0w3rdvu9iw4bpu+smy8oCIH3bEBpKwuU17rBhoGQoC7R/BPAlLuznAFYxxvZf7kqZttsCyqiY\nBV0454tr6KXLKxtQOBe5PQaNmhfTGBfCUHYXl1ejYWcoWWHDAPDQQ8DHP15oIuGSu7yW8R1ZyYDC\nAXyPMXYTY+wYy+8HAtiofd8ktyWMMXYMY+xGxtiN27ZtG1NVs223dXkNs8Z3ntle1IXY7ujyGvU8\nlCyGsWGDyO3xrW/lH7/UGsow+ple5qZN9vOa+44qyqvRSJZF9c0KGwaAb34TePObgR/8IL1PRnWX\nTJQvGYrVnsk5fwqEa+sNjLE/MX63+U5SISGc87M454dzzg9fs2bNOOqZa7stQxkm+2ue6Rf+SAOU\npWIo8/PFzrfUGspCGQrnopPPOldGlNfQGgpd08TE8C6vW28V/zduTO9j2JJrKCWgpI1zfr/8vxXA\nxQCOMHbZBGC99n0dgDGm8lyY7bZRXqN2eS22rN0FUMJQJLIEli5smCbZDXK/rGRR3qzTQQfZt+vb\nijIUYhzDurzyzr15s/ivs6kMK11ey2yMsSnG2Ax9BvA3AP7P2O0yAC+X0V5PB9DinD+wxFUdaLu9\ny2uxneKoXF4r4GUpZHru9KViKJR8cakAZVxhwxWtO3r0o9X2rHJHqaHoc16KMBSyAgzlkeTyWqlR\nXvsCuJiJiCAHwAWc8+8wxl4LAJzzMwFcAeA5AO4E0AHwimWqa67tti6vcTCUxWgKu0uUly6MLxWg\nEEOx5arSbTFRXnmzygedryigTE4qdleEoRjXO3SUF13HxIQq13GKMRSykqEkbEUCCuf8dwAOs2w/\nU/vMAbxhKeu1ECsZyiNMlF8OQNkTGEoYAtPTClAe8xjxfwiGsmANRddrdEAZEUNZsmzDpYay51vJ\nUEakoayA0VchGyegZDGMYTWUlRo2PDUlPjsOcOCByTJs5Y7S5WWrbx6YkW3aNHAFzCV3eZUayp5r\nuy2glFFeCzOa8Oa6S89QBrm8lpqhDCvKT06KzwccANRq4vMQUV6LBhSzzeeFDQPA+vUCzGlBtQwr\nU6+UNjLbbV1eVOFRzEOhmc6PBEAhhrJmzdKth7IUUV4L0VCGaUM6Q1m3DqhWk2XYyh3VPBRdQ9Hr\nO8jl9YxniP8DdJR4UPkI0FBKQBmz7ZZhw/o6EaNgKPTCPpJcXmvXLl3Y8FJrKOMKGyaGsm6dSvez\n1BqKvv8gUZ4AZYCOEnsqx+3yKjWUPc+uuAK44Qb1fWiX10UXAbffXvyEV19daLbuUMXahPTPf17F\n3j/8MPCpTwngufBC4LbbsssaFaDkMZRzzgGOPRa4/HL7sVu2iFnNb32rSJkxhF1zDfD97w9xgAVQ\n/u//gG98Y6jTinv7iU8kc0YtgKGcc442gNY1mK9+FfjNb5I7n302vvuVbfj5zyHcOPSMgVyX144d\nwCc/wcE/9vHkeusL1VDWr1cMhTrHr31NTSYcxFDuuAvXnf87fO97SJ4/y+VF7dMElCEYyi23iGQF\n73638a7fdx+i664HMBxDOecctSxMYStdXnueve1twGmnqe9DDwpf8xrgc58rfsL3vhc46aRCxZ55\n5sDdhJmA0m4DxxwjwAMALr0UOO444N57xfbPfCa/rOlp8ZlmdC/E8sKG3/xmUYdTTrEfe9llIu/S\nRz86JDoA73sfcMIJQxxA17jXXnHHdPrpAu+Gst/8BvjP/xQjFLIhNZT5eeA//gO44ALL8a97HXDG\nGWrndht49avx9v+K8MEPQj3j3/0uXbbRYV16KXD8fzL89i2fTYL6sFFes7PAX/0V8Nd/rRgKlfHa\n16r6DgCU6KdX44MnzOGd75Q/FJnYaKuvuR4Ltb0jjxT1fNKTxPdt2/D5z4vn/J73JHNb4stfRvg1\nMZooylA6HfHczj9/4K7KOF8RLL4ElBGb7ycHikMzFLOAQZa3VOpCizXTUJgdFhXU7Yq/vMyrUSQ6\nVyA56W9Yy2Mog0RpvX5Djt663cFad8J8X2RUnpyMOybfH7IMQNXZxgwKMhS6LfEl68f7vtpBO58f\nMLGZfksVkj4/7dLCbHLQMOw8FNcVgP+3f5t2efX76kSDGIofoe9pt65ILi/9u15f/Vrp9+c8R9ST\njvO8RNGJx9DtCmaC4mHDhH/DdAPWei6DlYAyYqN3Vf+u/x9oUTRc7+N5hRqpWa+BO+vlmy4A+k9g\nlpflNgyBVavE5yHWkEhZ1uhLv19ZF6ifd8iXLWsl2EzzfdEx1uvxfQuCBQwaqc76gVlhwxkaij7Q\nThwXhukF1OT5wlBupt9snbFxQ2iXNppJkBrW5aUvk2CK8r6f3Q6NaoWoIPQj9XORXF5Z9bUBKdWT\nsfg567slHoPnJQGlQEPIwr9cW0jyzjHYipzYuDubiQdDM5Shen4U1iWGKtZkKFkvMjGOQQxlclJ0\nsosBlCyGor+9Y2AoowKUoYMyqM76gYMYinH9KW1Zb4wmoMjzRdEAQKnXU+enXVqYVXXR617U5aUD\nislQdEAZxFBQReDzNKAMGzasn0v/ncAOKAQoIRpxvcYGKAsJmhiDlQxlxJbFUICB85+EjYmhBMEQ\n7SyLoZgvMgFEHlBEkXgBZ2eHWpQoZVmAor/8eQzF9MkXtKylxTPNAihhOCKGMkhDGYahxFQkeb4w\nYnZA0UNsx8VQ9I5aZyi0nv0QgBIGXN2mhU5sNM9hMhQ61vMSjyRxmqViKAuZJzQGKwFlxEbeBDLb\nADPXhl12twCg0M8LYii9XjZDKQIoYSiS/jWb43F5Zb38urXbwOrV+ftk2NAMhVw3BCicj87llUV3\nMzSUTEAhyjQMoOidbx6g6AyFztfvDx705DGUrHDePIaiv4fDaij6vBkbQ9HrWYihCHAcFlCGZsZm\nPZfBSkAZseUxlIFEgvNktEYRKzCE1t3QhWwcDGWxgJIV5UV1q1bzGQoFBiwlQwGAfn/pGMoglxfd\nO7qXCwWUDJdXiqHoz2qQwpzHUOjc5gUZ10vNMkIFoQ7io9JQclxe+i3J0lDG6vJaIRpKCSgjtkUB\nytBUAoUYytCAUlRDoU5vkIYyCoYyyOU1PZ2voey9t/i8VBqKFgG0IIZC97QIoAzLUGyAIs83EFBy\nXF4pDcUcmORZHkPJApSBDMVIR69P2NUPKKqh2Fxe9TrQ6yWqkuvyKuCeLjWU0mLLA5SBncqYAaVw\nO8sCFBOZqNPr97M7DHJ5zc6O1+WVByjttgKUpWYocvRqTmkYaOOM8jLBQjtfGFXyNZSFMpRBgGIy\nlDyX16BcXqgigIPQdHnpO+mf8wBlCIbiumJTlsurFOVLG9pMCWQohjJ0zw/h7x23y8vMbWQyFPOz\nbrrLaxyiPNVtejpfQ1kgQ+n1FqGhAAn/+lCRXsO4vDIYSmppdJOh6FqBPF/E5Wb6rYCGQrtmaih6\nOTYj5pAVNkznNi8oT5RHNc1Q9J304xcjymsaCnnORhU2vGANpXR57Tm2pAyF0GtAT6VHXhayYV1e\n5mfdRiXKD3J5TU0V01DMgIMPfjDzOM7FaRccNizrtyhAKRI2PEBDKeTyIobCDYZizjIfhqEUBRSq\n4CJdXmoeCjEUnrwGALj5ZpXxwffFecwQ5QWGDROgLEuUl41JLYOVgDJiMwFF7w8KM5SiLSmluo6m\n2KFFeSCbfYxalM9zeQVB2q8UBGL2tk2Uv/pq4L//G7j++qFOmWsZLq+hy8nTUIaM8spkKDYNhVfg\n+0DUM/bR56EU1VCKurxsHXURUX4gQ9HOTYknP/EJ4A1vUAc4TnoS5QIYShiqUyQeQ683tMsrK1NM\nrpUMZc+0JRXlU0PQ7DoBC9RQioQNm591I1F+djZfaxlkRTQUs+6Amnw5Oys6Dpt6mnFjFux60AFF\nE2yHes+Lurx8X1VwkIZiTjTMYCgA0O8anWsQiOtynOHnoQD5KextHfUiwoYjVBDAQRAydRy1j1Yr\nORGUrkkvb5iwYTkPZRwur1JDGYExxtYzxn7MGLuNMXYrY+w4yz5/xhhrMcZukX/vWo662iwM7YMa\n87PV6IUv2iCGBJSxMpQiLi9g4TrKoLDhmZlk3cx6EaAU0SSMoocCAouGsiCGUhRQ9A58RC4vAPB6\nhv6gd755Lq8VxFDCiCmXsA4ovZ5yF+uAssiw4SxAWeg8lN2RoazE1CsBgLdwzm9mjM0AuIkx9n3O\nuZFrGz/jnD93GeqXa4tiKCvF5aW/OIvVUHSXF+23dm3BimhWREMBxEXSmw0oAGs206PrjGghs+hR\naSiLBhRblBcBCmPFXV4FGIrXNdgMXZfJ8oBiDCUPUAYxFBNQ9OfGubh2bXPoNhD4DqKIgfc8MEAB\nCt3XXi95TXoBnqcGH0OI8uTyytJQQlSHChseOhjE9nmJbcUxFM75A5zzm+XnOQC3AThweWtVzGwe\nq5Xk8hqaoUxNjUZD0RnKQnWUoi4v8yLpfM3mghkK50NmizbmoQzNUMIQ2LUrfZCtIGIEzebwYcP9\nvtKcSEORo+m4/9cBhQTsFEMRZbTRBJ9fQJSXbeSvswZ6pr4vLibjpYqbSG0CgRwrhx1jwEHtgdJI\nZ4nytH9BUT6PoZQurxVijLGDAPwRAJtq+gzG2C8ZY1cyxg5d0opl2CBAGXmUV0FAGTAQzz6AUrBn\nMRQdRAa5vGZn8/cbZEVEeSAfUEyGUhBQcnZJ2yiivPQ0/0VdXrOzw7u89J0obJgAxWQo5B6yMRS5\nbwgHnS5TPxR1edlG/jpr0M/X79s7eWiA4k7EwBjMG+2D2kOnk2YoOuAS3SjCUKRORmOIZZ+HUory\naWOMTQP4BoDjOedmL3QzgEdzzg8D8GkAl+SUcwxj7EbG2I3btm0bX4Vhd+0uyOVVtOdPTTTIL3Zo\nlxcxlKz4f8s8hpTZXF4LsUHzUGhEOUINJUubzbWceSiFy9Dv0aCwYWIoFPSg2cB5KLJ+xIgiKDDo\neZqgDeRqKL2OqmO75yrWMwqGoru86KIGAUq1phhKV14raWx0U2jpBZsovwCGQo/ddVdAcsiSoSSN\nMeZCgMn5nPNvmr9zztuc813y8xUAXMbYPrayOOdncc4P55wfvmbNmrHWW4/qtK0lv1wMZcEur0EM\nhaxeH78ov1CXV56GMgRDKfyOjiJsWL+XNgDUn7fOUIw5SZlRXiagSPdaWJtUm/sZgGJjKD2OOkRH\n3UZTddpRBNRqyXJsZouesonyVE4GoMTAXXEQMlFW0JHXSgBBNoih2AYoORpKGIrNtdoyTWzMGsUu\nsa04QGGMMQDnALiNc/6xjH32k/uBMXYExHVsX7pa2s2M5jS37XaivKmhZAHKmjXF5qEAi2coWVFe\nY3Z5LQZQhmYotnXk9c9ZDIXOj/jUyd1tUUyep/STNfupzX0k99X1BosovwaC/SfmooShch2NQpSn\ncjIZimBGEXPSGgq1D7JRaihhiCDgqFbFo48BJQiAMHxEpV5ZiVFezwTwMgC/ZozdIrf9N4BHAQDn\n/EwA/wzgdYyxAEAXwIs4HypT0ljMBBRzLaLlFuULtzOdoejzULIKWr262DwUYHwaih7lpVu7Lc4/\nNZXt8hoQ5WU7baZZ5qGMnKFkaSh0fnnuwhqKZBTh6rXAZrm5X4nrDyCpoZiifJ/hQGzDJqxPRnoR\noOzcmT8PZVDYsP58PM8+lwiAT02EOUpDIYZiAorOUMyw4V6vuIYihZMw4HAchlpNq5J8AMMylHJi\n4wiNc34155xxzp/EOX+y/LuCc36mBBNwzk/nnB/KOT+Mc/50zvm1S1K5rVuBD39Y+Yh/9CPgiitE\n4zzlFOWvBRA8sA348IcRhgrnrM/5ppuA//kf8TmK8EW8HL/uP75YfbIA5eKLxSzwrVuBU0+NU1CM\nhaHU68DsLO7cMoMzz9TKuOQSUQdyedXrwh9w4YXA17+efe4oAt7/fnzxjHn8+tfAXT+8B2e+6MfZ\niRE9T5RLrhVZt09/Grj3U5eKe9FsitBSbXR9113AmT88BADwqSsPwb33yvK2bQNOPRXgPMlQ3vRm\n4JWvVH8//KGq7wc+AOzYoc6fpaGc8C7g9a8HtmyxX/ull+J/z74JF1yxKt70pd8cjl/+Urs3+j24\n/nrgU58SnwlQNLCIm8fcfPI4k6FQyPA++6rNWS4vk6HccAO8bhQzlMRclChSoU+ynO3bgQ99yEho\noHXUp58O3HMPxPOqVLIZCt1fC0MJWRUBF53+/Kmn4xScBK8xi4RpGspVN0zgcjxXBQBEUdLlFUUi\nRQ89N9PlBSDwBQYmXF7ymhPzUOgZPvgg8LrXibb0qlcBN9wgtvd68K6/Jb7lt9wCfPnLyarjqquA\nyy5T3z/zGeCOO9R37Z789KfAZe+6UZwjjyWOyFYcoKxou+wy4O1vB+67T3w/9VTgPe8RT+3kkxH9\n4qZ4V/+yK4G3vx3hLhWXbyUSp58OHH+8+ByGOBan42zvZcXqk+XyOuEE4LTTgG9+E3jHOxBselDU\naVgNZWZGvPnmnAi9oHodmJrClzf+KV73Oq0tn3gi8LGPKZcXADzveaLhn3pq9rlvvx048US88S0u\nPv954Cv/ejled9Gfw4cxitTvQb2uXnLfx9wc8KY3ARe99w7g7ruB58rpStro+itfAV53wbOwA6tw\n3FeeivPPl+VdfjnwjncAd9+dZCgXXAj84Afi7ytfESk8AHE9J5wgjqN747pqhNvpKGJx4VeBz34W\n+O537dd+4on47Cnb8J9fPCze9IarX4SzzkIydpkKPOcc0bn88R8Dv/d7YpsOKPPiYYT3bkweZzIU\nApS9lcbo+RX1OyCGzY1GSpTn53wBXuRi7d7iXCmGQsxAgszllwPvfCfwu99p1y0bTTdw8cY3qvFV\nfC4boJgZgrWPfqQ6/Kt+tQon4xRcW30WEqYxlNPOnMa7cIool+pO9Q5DYMMGkaLn0kvFNtPlBSAM\neSzKm4BiXQ/le98DzjwT+M53gPPOA84+W2z/2c/g/fK2+HrOPBN485uTVcdHPwqcdJK6jmOPBb7w\nBfW79nw+8hHgXZ9fL36vjL+7LwFlGNMpMaBGT7LBhztVuKffkdt8NRSzAkqnk1g/3IeLXlQrVp8s\nhuL7okzyjUvm5PsFU6jTm0kZeik6LouhNBro+U7ypyBQbxY15K99Dfibv8lHNvlbz6+KWxtJ1wUc\nUU4WoFDucN9XGVV6AfCSl6ghnja6pn06EB1/LFvQD71ekqE4E2Igcd99wDOfqQ6g/fXvriuGqjJY\nIQ4bptctS2/yPPheBD8Q7CCcWYVdfkPcRlt0h+8DBx4I3HijylWmayhytntUUEOJ9lETTj0YYnqr\npeby6J14PwJHBWv+7e/FbrqGEkXiHkxMxKBlzXYj6+WzWvIS6FxFGYq8b16gOvzu+4QM29v7ACRM\n01D8oAIfriiLno2+widtm5dMz8ZQAiXKp11eVfXfHJRdfz1wyCHqHK0WPNTjXazZrukHQN3r7ZqE\nbES++QFTDH3MVgLKMGbmQqLREwFKa1e8q9+Vo8NggMur01FuJQkoHmrF/KB6x6Ajhe+LF5hGnj31\nQhaaC0HnJkDZujW53QSUeh1eUIlPHe9L9dNHRqm4SsN8HxEY/FACypRw//hwrZlu8wAl9HwVDAAk\nGArt04VwycSyhTYrO8FQ6ioCKpHokupD3wlQgHgNmNjlJTuWTB3J9+H3OfywAjCGuZkDVJV0/z0t\nrqKfi1x+OkOJAcUI49UZSq9nZyhQLru4zrOzKYZCrjEKoEwxlGo1sRaONVhP3iCKzIqbB53L1FAs\ngCIy4Iu69EPV3uIs/JGLhHU6scvLD5gCFHo2+vo5+twVwM5QpChvdXm5UmepumlAcd1ke2q3E4Bi\nxiDEdaLnYksMqt2vMAT8kIl6Mm2O0JisBJRhzBzhmYDSno93DXrioUbhAIZCL9/cHCI/RISqaFBF\nFPSsiRI5gFLI7WUyFAKULIZSr8ejwgSgUP30F7AAoPShRqr+pAAEH64Y6ZpvV6+XBJQgUKNgnytt\nAchlKDZA0W9voIXUJhYLM7PArzCRAAAgAElEQVQG0FAViDuK2OU1CFD6fQUoMzNoV1bRZnXddJ0m\noFhWd+p1paZAtywrbJjayV4q8r4HNdM/rrOFodB8lakpYKIeJjUUPWQ8j6HILxSZlctQaB6K4fLS\nd+kHFkAJjdE5MRQdUHTw0Jc7oG15DCVk6bBhYig1sU9UcZODAUA8N709FQGUIFDPRU91A6QGXUEA\n0Z4IgMdsJaAMYyZDiaJMQBmKoQBAuw2/L/b1UC/W8+tDaB2tTEDx0pO/co3ednqp6EWyAUqjIQHF\n6AyiyM5QHGcgoOgvlN8QE9JiQLGFDRsaSjwKRjXJULTRdRFASTAUOcoEUJyhyP0SDGVyMh9QyM3X\nbIrOGQagEBOhwQxdN21PhA1LhpKVeoU+UztZtVpttjEUCr0Ow5gRk9ZSrwPN6SjJUCxLF1gZivwS\nVtzkJdAAwHR5BUGKoSQm02uAQlXxIgNQNA3FDxfPUIIAcdhwyuXlSECpuuq+0E6OM5ChpAaiNoZC\nZiyAJhhKCSgr02wuLy01RMLl1ZMvSVGG0mqJETUWCChmvHyrtXCGYrq8zO02hhIaGore6E2Gkse+\ngiB+oYIACLimoTQaoiPT3XueJ7ZrLq8Y9+CkXV5GB0Qur1jW0OYiJDSUmuHyarVEPWh/+m4CSquV\nZCirV2drKL6PwOeIeAXRzCxabBVtVo1HBxQK5dW3W6K8wkFhw60WMDODsKaSanqQIE1D5Lk5xVCg\n6kMur0YDmG3y9DwUYijymq1TmQyGEv9GLkqToXCeApQE5vgWhhJkMBRHpLkPIIGSno2+wmeehkJh\nw6FlYiO5vCSgEGAmlnUll5dFQyEisliGEpSAskJtgMsr2qVGCzGgaC9OpigPCIbiyZd0FAyl349d\nVYGXbGADzXR5mdtzAGUUGkqCoUihNdZQqGyyPA3FxlCMDqiohhK4WgbjZlOUo6cBabfTbimbhpI3\nZ6ffj6PZgubexRhKjsvLk+6olChv3j+pj1DHB8g2ODsLfSZ9rKEA8X1MMJQmszOUQRoKMRSbhqIN\n2ACosnMAJYxsLi9tUAMkNRQf2QxlCJdXloYSOeKZRRVLwsscDSUTULIYSqWSGrCFIeBHFfXujNlK\nQBnGqNPWkyTpLq859XCpEx84D4VeEM3l1UN63W6r5QEKAGzaJM67BAylFxnuikUACvnvfV/SdWgu\nL70eQBpQdA3FxlCKuryMKK+Uy4sO0jUU+qxpKFFrTqW2Yq5wI+YCiryP03uhjRnaPBhQbAzFTC5g\na4AEKM0mItcAlGYzIdonGIosKwEos8wuyhfVULihw9nChqkDzQAUB8m2RVWhKMTED+Ty8i0ais3l\nRYBiE+Ujls7lFTOUHEBxHAW4MkQ/djfKU5LskrhfxFB1hmKZeBoE0oVaMpQVaINE+TmdoQTxLtSf\njpWhmKI8ANx/v/hpoRrKMAxFYxVxfWyivCV1R8JSDEUDlDyGYtFQAjiZojxVoaiGkmAo+qx/naHo\no04AaDYTulpUNUajusm07AlAiWZUlRakocjw40GAIkOCw6oKV48BRdNYYg0FUAxFBmPU60BzVSU9\nsbGIKC/rFTBjUGIT5TMAhcprIDkjX7m8LAwlBhQLQ1m1SpWv62NAiqFEYODcIsrLkxNDCU1AqVZF\n5FWzKRBjfh5ot1VABBQ5TPQdej+kMxTLxNMSUFayDdJQEoAiU3oH6l0fqKGQ+34xLi89MyXVqz8k\noFCDnZpSnaO+3TIPRafp8b7mPBRgMEMxNZRILp4Ex55+xeLySriYCoYNkwSSpaHo7qBEoktdQ7EA\nStCaT5ah+8t1k8fGOsL0KrS4xeXlGn74PJeX1DcieQ+tDVBjKCmXV7Mp6rVzp7puk6HIcxCgtDCb\nzVA4V8/GFjZM6VKoDdkYCrGEjCivQoBSqSQ1lEDed9JLpqeTwG0+L4OhUL0zXV5SjI+YBij6s9Pb\nk6ahAPZlcfQ2amUohssr4CWgrEwbNA9FmxVP7qso4nG7SQ0QOR99lJdl9B8uVENxjBH+sAxlFBqK\n7vKiyVt6x6iFDd+BQ/DJKx+X7fLK0VBIEimsoQBpl5cJKLOzih0AggFkMRTZC8UMZWoV2tzi8tIb\n0wCXV69fgKGQS2t21s5QADWx1aKh9HzFUGZnYQ8bnp0VnzudfIaSFTas70yAkuHyygYUrQ3uvbdF\nQ6mBB6Gab6Ovx2I+L4OhUL0zXV6VDIZiAoqMzDRdXtotSlyzyVBu4k/BuXP/nBbluQNeKwFl5Znp\n8jLDhnVAke6rMMxhKHqPZQJKkZ7fNg/F0lmHfdXAhnJ5VY0Rfl6UlwkoUZQ9DyUIsqfsJwCFK0A5\n5FDghS9M1gNIMJQ/xk04/qJnqP5siLBhQPYbGqAkbm+WhmJzeWkaSqDlXw2rtaS/XDcTUCZn0ebT\n6idblJcNUHSXl0zwWFRD0QGlh0YaUAa5vJpy1caOJWxY3iOrKG8wFGvYMOlnAzSUOpL5quKwYV9r\ng6tXGxqKrJMfJcOjqaImoBgMhZ5xJkOZFgODaGpGlam7K3UXqgSUSSau08pQ9H5IYyiH7/wBXnn/\n+1IMBTCiFMdoJaAMYzaGwrkaicyrHijoK0DJZCi6/3NUDMVynB7lNZTLywSUvHkoNoZC5ZjzUPRz\nmKYDSl8DlLO/COy/f/pYTUPZJUVsuq0B3OQ6GJaw4TxASaZe0QBF7wD0e0IrLeoaClTnEzOUmA4l\nrxswACWaVj8V1VBkb8a5EsyjiGWvY6xrKLYoLyAJKKbLywCUCFV02ppfy1j+OVeUtzEUHTjr9TSg\nkKtQlmcCSsxQ+ky1w9WrUxpKXJQ+gZMKHsBQ6BlnTmyUwQ40HyUeiOYwlGnsinfV/ycu1tRQyAyG\nAgC+WwLKyjMboADxKCERNtynWcos4fZOmO7/XKyGYmMP9NNCGEqlogRDfTv9p+SHBkNJaChkJkPJ\nq4iuoXghAhkCGgRIdWYA0vNQoHlc3EYy3USOywuQrvJhNRS9LuSS0zUUk6FkLTQme6FYQ5lsohUO\niPLK0VASYbScZefc6XSEX6XZFKlBpGUCislQZDhuQyM0rTktCkUflGht3Bo2bNNQyOVFGZwHMJRM\nl5cHVfe9905pKIAcBFLOMv068zSURmMgQwm5TA7Jmbpem4aycyfC1i6EcDDNtWWgM+5XSkMhszCU\nElBWotmivID4oYa+1nEToETIBpQMhtJDY6QMRQeUQhoKzdICVKeihyP6vsrGmqehkJkaSkY9aXtc\nlhfBDzU3SBag6KI8tIn9rhF7nyPKAwZDkWHD8UC1qgEKLSerMxQAeOih5DXOziYYSuTUkqNR3UyX\n10QT7WhK/VQ0bFjWP9U0bIywWlV1np1FlKWhbN0qgHl6OpehxMRtTus4aR6KvOYFhw0TQ1mgKN/r\nQSXtnJkR7550H8QMJWBKQ6E2a7q8GEu2Z4OhWGfKc209FCrTxlDuvz9OykkMhWyQhqI7UHmQft9L\nl9dKNNs8FCB+qHrnofuKM11e+uhCe9kKaygLAJTCDIU6DmrsU1NJhkKuJJqHos0dSblXhgSUuKw+\nF5OyMBhQoqoFUBwDUHJyeQF2l1e8LIZelrzmhIYCAA8/rM4DpBlKxc1eaMx0eTVm0A6n1E9DivKJ\nppHFUKamEuwjnskNiyg/MyOeY0pDceJbEmPlvNZxFnF5DRLlbS6vDEDJdHl5EGVMTCSyHycApc+V\ny4sxcX7fV65MIJ2x13GEaxUZDKVSQSij7CKdodg0lE2b4sFULqDQl14v7kMehgrxD/0otWvJUFai\nZbm8LIBCGkoU5bi86OVwnARDiVCNk0vmms3lZQMUrYEV1lD0BIeAcHHpbjUDUBIMxbxQcx4KlWEz\nU0OJchgK+aLrdWzZrl702OWlu6moHtR5FdRQyLOnC9YAVLSWfh0jcHkRoASN6RhQMsOG9U7JcHkl\nmkbE7AxlclIl/mw2444PsAAKfTYZSmgDFEfV0RDlc8OGeUX/amcoC52HQoAyOZnIp8YdV706xFCo\nvo6jxZMjef1kjMWZqK0aSr2ulrHhGQyFGO/GjcUAxWQojoONWB//TNkl9F19R4tSHKOVgDKM2aK8\nAOXysjGUKCfKixjK2rUJ/zIAeJ0M0Vq3gmHDBG56vXKNfNZAkqGQ0M55ElDMeSjmhQ7DUHQNxed2\nDcXMmluvY9Nm9RLFonx1MEMZpKHEDKVqgNPsbCENJSnKuwNdXvEovT6NVqABypC5vFIMxQYoU1NJ\nQJG71NFLu7z0ThbQNJQ0oLQ6GovSGUqrtfCwYdJQhgwbjqO8SEMhhiKRJqiogULghSpnGZ2fWCeZ\nZU2RQOZAI5eXSKWPOKQ9FtazNJRqVTyLjRtjdl6IoZCGsnYtNmFd/HPfLwElZYyxv2OMbWCM3ckY\ne4fl9zpj7CL5+/WMsYPGXqkBorwdUFi2y4t6vv32S7gDgAUAShZDqdcTDKWwhkKdN9FxGqpTJ64B\nSuTWY9qfcM+QLVRDCVi8+p6VodDws17Hxo2qiDh23zFYhSVsOFNDiRmK1MKKMBRTQ0m5vHI0FNPl\nVZ9GO5hUPw3SUIz7mgh5jjLWMp+cTMwxoV0m0RHPgEbOW7cmtTRAzUPRACX25nU1FjVM2DA31tQZ\ngqEMDBs2GYo0v6IGCv7OeYEGdCGOowCFzmcyFAChnKNELi9AviaSodC1xjnGzGcHiHtU1OVlMpR9\n900wFB1QYqeC6f4dk61IQGGMVQF8BsCzATwRwNGMsScau70KwA7O+WMBfBxAzrqyI7IsUV428ki7\nnfGqe3kuL2rtBCh9Ra1pcaRco0l9euFmR71mzfAuryyGQufUv9fr8JhqrFZAGSbKKzEPhSVdXrpQ\nCqjnUK9T2jIAGkOpWFxeQ2govR4wJQElKAIopoZSqyHUxNBcDcVwefWqU5gPJ1BBiCAAIn8AoFSr\n4v4YDKWOHiKObECh7RpDmURHjJSp4w2CNEPRXF5VFiZxw6urfapVcYx0Mw0lyudpKAUBJVNDkZYA\nlFYnvhfx+emZ0gpiVoaSdHkBSUBJMRTTXQmIdqEByhRUhgU6RJ3QmCk/PY1N7u/FP+vp+wO5fMYj\nnaEcAeBOzvnvOOd9ABcC+Edjn38E8EX5+esA/pKxMS9JlqehVCpJDYV8xUUZiufFa6gAgNctsLSi\n7uQ3AaVSES/EXnstDFBsojyQBpRGYzCg2Oah5GkoTDR+P2Tw9U7GZCgaoOgMJdZQNKE+PneGhlKp\nZIjyDTmfqJIBKHkuLwDBlMo0EFZdNeofoKHs6Ip7uroq0p7QRNnMeSj0mwEoE+jaXV7VajIDbbMZ\nN6FJdMQzbVgmc6ZcXi7qVfE5Dn7ra4BCz16mnMkNG7ZpKDqgNBqZLq8sDYXkj9jlZTIUqGfl75xP\nX6sJKBaGEmgMJTFeMgEFhihvMpQoGk6UJ4YyOYmNzmPUdQRphuKb7t8x2UoFlAMBaF0ENslt1n04\n5wGAFoDVGJNdey3whVufJr7YXF6rVyddXgUZys/xNJy39dkAgGBORX1lAcoDDwAnnyxfFM/DlZW/\nxzfwT0nBHBCx9s2moNxB0uW1c/M8Xv3kG/CyF4fY8NFvAT/6UfIkUpS/7Tbgoz/6I7GNAISuXQsb\n1lNFBIE4/gt4BY7GBTgaF+D4cw9TL0QRDcUV5wpChoDWlNc1lGuvBc45R9Wl0UgwlPmrbxbHMAME\nLAyFXF57752hoUxIAJIayoc/DNx1F9SMd0OU34DH4eNfV/7scErN44kqImz1LOf1uOmcW4Cjj8b8\nv/w73vHkK9F9UK4XIl1k23eIV5MApS8XyzI1lC3eKpx8smxbtKD5hg3wvvxVAAIcQm5xeZEmQWYw\nFA+NxO+X73wWjj4aOPrUJ+NoXICXvaCD2751F7zIRaMqbqbrAhNOH21fjobJ5QXgq+xf8YNL5+Hf\nKxKWBgGAs84CfvELK0P5xS+Aczb9LYIAeOP/HYMX3/1+XNt58sAoLxNQyLIYSkJDaRmAojOUtWvx\ncRyPo+c+hy99KVl2qGkoNUe8a/3N2+I5UsrllaGhaOfMApT+cW/Dm37/+3jxUR1c6z9VXVS3C0xO\nYhPTNBSPA296E/DiFysNZYkAJc3fDGOM7QvgAwAO4Jw/W7qensE5P2eM9bIxDdMHVGQfMMaOAXAM\nADzqUY9acIU++1nguz//R7wSsAPKqlUIt2mAIl+OMGLZonyng7NwDK686tn4dwD+fHr5VtMuuQQ4\n5RTgFa8ADvJ9nOa/HlvQxAsieSy9Wf/0T2Ik9otfIPRVWb4PXH/ebTj7l0cAvwT+8Hu/wX896zrg\nL/5CnUQylHPOAT563h/guL/6OzhPfjJw2WWKoTz+8cBzngM8/enwHq4nykcY4j04GS3MooEetnxr\nPxx3H/CYx6CYy4sYSlSFX7EwlDPPBO67D/jhD8X3iYmYHADA/L1CywhnViXLzhHlV62Sg19tHkq/\nD0y4ElAqNczNAW9/uwDzt09OigOoDczMANu340Icg3d/4tF4w6mifw8mZuLTU1ju29hpeNmOb+KP\nb34vrtr+NJy6/dn464tPx5+hAi7HeJSPcVVFhKzGDEW/f5zj23c/EadcCrzsZcBjazXRNi+6CJ0z\nrgPwQkxjFzw+aY+8e/azgXvuAQ45RCywJS9losHRxzTw6CZw5JHA9u04Y9s/48c/BR594Gqg/gzc\n8cBBeNwnf4pOVMeEo57lVM3HfKeulGnJUN7dfycObv0v/IcfAHCAONdb3gIcfXScAUEX5c8+G/jG\nrS/D/1v3bZy+8SgAwPQMcGT4CXEiAhTJyEjaywKUXg/AK18oAmCOOAL4wz8EGIP/xMPiffxd8lhd\nQ6EHsWYN3ouTsKO7N379YeDlL1dlB/Wp+JbWtmwE8Gj0v/tjAX4TE4goY7AZNqwBG/7hH4D77kOn\n+afAjcDs7x8I3KZ+vvOmFj699a+B24EmXoYjca1iKBMTeGjmMQAtL3/3ZuDT3xVZkHGBqKMZVDIm\nK8JQzgPwXQAHyO93ADh+XBWStgnQVCZgHYD7s/ZhjDkAZgEYIRkA5/wszvnhnPPD1xBtXYC121Aj\nL9s8lImJJEORVDriOS6vbhc9NJRbp4CGksikHQRoY0ZEeJgur5e+FPj4x0WuISN9vZ6Kxeup1DGx\nSYZCo37/sivFiwiofZtN4NvfBg4+GD2eBJSwH+J+HIDX4wx8BG9NVEtft8Rqvo9eRSZsjKrwuUWU\n73TEjSC30eysGAwy8VzIjaW7mwDkhg3PzMhL01xevg9MuFIwrrjxY/d9qBlsOiPcvj05HwdIJJUk\nQOmxSfj/+lJgwwa0jz8JANDeGSXcL5TDqckEoMQMhe6frAzde8+DYBSeB/R68eJce2GHmFhnYyjH\nHw9s2AB861si/JWivJ72R/DrM4KVXnMNcPvt6K3aH0ccAWy4y8WG+fWYxDzaHQftcBqzNdWJu1V5\nHb1egqG0G/vC2/+g+HkG/UhcJC1VPTOTEOV7PcDnDvrawlhtaM+TAMWYyGlqD2SeB+Ckk4DXvAZ4\n0pOAX/0K+OUv4R/8hHif+P6bIdIAsGZN/GxN+YtYqOMAtTkxsvFbHZV0k6Lcs+ahAIJRbNiA9tvf\nDwBY/d+vSdb/5A+k60nzUCYn0ZneN3Ya0CJtif5oBbm89uGcfxVABMTupQIhSIuyXwA4hDH2GMZY\nDcCLAFxm7HMZgH+Tn/8ZwI84z8o4uHhrt4W/2EMtHTZsARQabeW6vDrCtUChscMACrmW2tEM2phF\nu8W1H6A6nkYD4bwCDN9PzkvxPKQBRYrypEsk3E3Uq2ovg+7y8n1gyxYggIt12ARXLniU8IvTjjbz\n/YQm0420+S06oPh+YlKe5wEzFdGZxIBiYlbOxMapqTSgBAHQcKQ7hrnxbQoCrSzqLfbaKxFQEGsF\n2lyYsOKCczGapt9pkNJucSugzDDxod83XF6yMp5c2CwBKJ4XA8oq7EQIDVB0Ed8w2sVYljwuP/aA\nVatooo12z0U7mkazrgDFqcq2TzPR5XlaLcCLHLWkc0dSCgKUZjMxb9bzgCCqJOpByTLFiRzxZwDK\nJA3TDTObOJneDFOAonf4++wTP1sTUILJGbotcFuiTfZb3TiNSzwPJc/lRddorO8V1z9UdYkjBzWG\nIp0k4txytr0eYeibASpjsiKAMs8YWw3pTmKMPR1CrxibSdA6FoIZ3Qbgq5zzWxljpzDGnid3OwfA\nasbYnQDeDCAVWjxKo8FwG3LRIX1t825XLrSTZighr+TOQ/GqE3ECRH2+SBagUD2IobRkAsFNWwxX\nEjXWeh1hR0taGRjnCSp2hlKtKobiQ71ctK8OKFzzQwfxQpFYj40xoKQYSp6GogMKV+nlE4ACIEa8\nZhO9ngKUeUzFl5Ewx4ln8SuX1ySqLESjIS9N9mC8JxhKQ7pzwooClBRDYSx+m80UNCZD6RueyVZP\nXGtrrpLoABSgkMvLABQJ7HTvTUBpydG8YCgsDSiWaCUdUMzHkwAUAE3ModWtSUBR7cd1JTB2u7HL\nKwzF9XiRGzOUOJFqq6WyHWuR754nGAq9GwDQCjVAMaal60EINssCFB2w4vtvMpTpaQT1qfj9breT\n73Io3ZqOA9R2iHk9/VZXrYRpm4diivJ0jfL93mef5PZ+pIEDAZ+moXQ6ylNHgBKuPUAds0SAMlBD\ngeisLwNwMGPsGgBrIBjBWI1zfgWAK4xt79I+9wD8y7jrQRYv+Ywm1nheurdyXTFXgV4KApQBUV5e\nZVJl1NXnoWS8ALHLqy86xjbES7bxAQdP1AuhDsOYh+L7BqCgbmUoYbWGzZvVMbkMRVu8yPeBjZvF\n9azDpjg+vjCg+L4QhNObVcSQCSjS5TVd7QK+ApQUQ9GixHxfdVQOC1GvV4UOI+tFq1ySPqAzlASg\n0Chcvs0moCQYCnOSZUCF2LZ3VawMZVq6cFKiPDGUDEBpYy0YIjTRRsirQzOUgYBSmUO7N4kWr+NR\nOqA4ElA6ndjlRdfSC134XDI+naFMTQGzs0pAJpdX5MTBLVPVbjwvJ66/66YAZXJIQLEyFApX01Kj\neFVx7jXV7dgWrsb8vNotmNRcXju2iLLa3ZTLK4yMsOEhGIru+ovr2e2KC5MMhQAlzrZw4KMBOW81\nMKMUx2QDGQrn/GYAfwrgSACvAXAo5/xX467YSjMdUJAHKNJ0hpIX5dWrTCAIRUNLuLwGAUovRB8u\netIllMtQUIULlYVWB5geGulU6mGILdGa5FxJerlsgKLV1fcRz1rPZSi5YcPp0VQC1Gj+DlEhcnlV\nxfYOy2EoEMnz9NO7LIj74niCoRTBGxXFUFIaiu4Ll6PalIaiTSgLKxZAkZMA213H7vKC1FAoXsNg\nKD3p8ur1IHr8Xi/WUGacLhwEIlzVnBhpYSjUPuuapk5mAsosm0O7V0ebz6DZUMEkrosUQ6E264WO\n8u935I3QXF4mQwEECAHA6voutPvaQMPIc9LrARUWoVa1e+ODwJ7OLAUolLNMv0fNJjyp661xdsbV\njstuiEFdlUVwH3oAANBv9+JZ95kMxfIM2m0RQGkusGgFFBkwEDam4HkWhnKAkqF9M+JxTFYkyuvl\nxqanMMbAOf+S9YA91JKA8mAGoGjzD+jF4dV8hsIa4JwhRCXJUPqwWqyheCHmoCKINm6RDcbUUCSg\nNNCDj5pgKFrUVxZD2Riu07+mAUUbXZmAsvGhKhroYjW2pzWUxTAUc1S9aZN4+ScnBaA4ElB0N5lu\n8vjQS/7gsjAFKJRLrVFVKVFSGgog7oc2qy9PQ4mYE9++WEMhQOnVcgHFNzUUcnnpGgr57aSG0qzM\no4pQTLg1U7cMcHlRHXVimmAo1V3Y6tXFeSbuibc7DlMaimQo5MbxQkel5+8SokqGsn59SkMBgK4G\nKHf3NFGeXF6ahtJwAlTBMxVeOZhPWApQzCWjAeFSrUiG4u4EPFHtA+VEhnBCAIrjd1HbLgFl206B\nylmAkqOh6Nnz47oHaX2WAKXniPObgBLsp97hFQMoAJ6qfW4A+EsANwN4xACKfEcBQPimbQzFceLJ\nb64TwQ/ceCJTHkOhztOHm8zl5dnnaMYaihfFfnIA2LTVWLFPA5QADlz4YIgQBJWYodTgZQLKpmC/\n+OtAl5eXOBSbH6hiHTaBAWmGMkiUD4I4hbex2Q4oMjOs5wHTjuxk5T3NYij6BFJAMBRTQ6EOvM4E\noKTYhR5tpTGUlIaSYChuiqG0donRcKs/kdBQaP7eDBcjiFSUF7m8QkOUb7ViDaVZmUMFkQgWoRMS\noOS4vPS1q+hzyuVVnccObwK7MINmQz1Lt4akyyvFUCRz72oMZXIyIcpzrkhoNxD1Xd2Yxy9bLjjk\nfAGLy6teCVBlHMhoWjZASWko5gqfgGAoUtdb44pOXJ+XGocNd+ZQ2yb8xP7WnfGx1nkoORpKs5n+\nydNctPHAY8cOAEDHEXVOAMrUFMLmXuqYlQIonPM36t8ZY7MAvjy2Gq1A0+ltEZdX3SVAEY0gbx4K\nuXd8uIk+tpcBKLHLy4viSB4A2Lg1uYJdQkNBFVWEcOHD9+sxQ5nGLjughCE2BvvHXweJ8rrHzPeF\nnrMOwh01tCjv++jxggxl82Zg3bq4Ws2ZpP88i6HEczqkOTaXV088Xzfqw4GPABYNhS7ecTI1FD3+\n36qhtMVzbvOZDIYiAYXyM5mAIsVaW5TXLFqKodCJh2AopqaXmDhf7eCBeXHNs5Oay6vGxHUQIlar\nCUAJyb9PoB5FIixQ0xr06+9EMmNAo4MoYpjHlNCVLKJ8veojDZPKTM+ueY0+tNQ4su7iAmdjl9fa\netrlFTOULZtR6UsX5fa5+NjUqos5gELLsZg/6fm5TEDpVgSgUZQXMS3SdgChAS6FLWSmfAfAIaOu\nyEq2FKBQjL1umsurUROx+BRGnDcPhUTVAA58naH4+YAS9BWgzGInNm0zAMVweVURwmVBQkOZwnw2\nQ+mvjb8Ow1B8H9i0xUN+r2kAACAASURBVMF6mejAQTJMt5CGYmEoVkAJQ8UMPGDGTfYY1rBhILU0\ngE1DCeQ9cnkfVYQJl1cCULpdq8srFuU1Xc0OKPI/mvArqsemJThmIgkoGRoKhZPaAKUZtVBBJADF\ndFUWZChkKYbidITYD6A5oTMUGa1GiKADSqAYStDTCpfPUX9edP3dUDKUyW58n+L6Gy6vejWwXVbi\nGkzLdXnpDEU+17V1QU0SGgoxlLvvjAdQ/Ui5ywrNQ5FGLq8UQ5F1r1f6iskSQ6lYXF6zs4ksDUvF\nUAYCCmPscsbYZfLvWwA2ALh0/FVbOaY3nhZmBZj0DZHDdRFVxINuuKIDMgHFylC4zlC09Sj6A1xe\nvTB+uQ7Frdi4LZmKIjEPhQCF92Wkq2AocVZZG0Pp7xt/TWgGtrBh7XDPAzZvdVMMZSgNhWcASsXS\nXKWP2veBaTf5TDJdXgZDsYrysuNzeB8OAoSsmq2hWFxesR4wkKHI/2gmFkGKGYp0eWVqKLmAskO5\nvExAKaihAHJ9elOUd9UEwuakQgJHMpTb76ziQ3g7UKnEbbYXVJW2aK73o3W8+vXHDCULUBIuLx+V\nqnpvzOYyNKBoGkosytfF89BdXvF6KHf/FjUKftHmtMTzUOj6omhoDSXOBFDpp0T5LhPnTwCKwVB0\n9jtOK6KhfET7HAC4l3O+KWvnPdFSDAVILt8LJDSUhhsKhjLZBDr566F4XDxoHy6CEJjCLsxjGl4/\n3XnqWo7f59glNZRDcSuu7T5TNMYMhiLWlfPFGiPS5ZXPUFRWgaIMZaraxfbtEwhDhjUQE7wWpKHw\nGqaqXcyHE/pm66gazabKqF8TOhGlL8l0efWSSONCAQpHADY1BX9e3D83kgyF57i8FsBQqG7UMbUw\nK/QW+Tt5jKYHMZTADigtzKIZ7Mh2eVnuJbVP0+VF/xMMRWODzQl1o916FW24+Np16/Au/CuODT6u\nWHVURSh1xdQCcs0mAu09o+vvRpKhTPXi+wRALT5i01Cg6tvtCs1/ft4OKIU1FPlc1zQEdUowFJlt\nuHrXHajJB0jRfguZh5LHUBKAIhtPx5WuRz1suNlMpP1ZKkApEjZ8lfZ3zSMNTIDkaCQTUFw3Tq1R\nrwYSUGboJwD2KK+efGF8uPADhgZ6qCBMiHBk+kqkuoZyKG4FIKdl5GgoDgL4/SjO7RUzFBoxkYUh\nNnbX4AA5L2rgxEYClEo3ftFoxvKCwoYjF1PVnrnZDiizs2pZFDeKR4jyMpJGLq++XUMBlKBJL6Ab\neYKhoJoOGwbSGgozwoa1+P9IYzlWhqKlGKc5szORaHyxD92chyLDSe0ays40Q1mAhhK7WxKAovSq\n2Wl1o926mE/Tl+v5zPmNROdLYG9G2pkaCl0/RewRoOS6vCp+oolQfc28proV1lAka15TTwNKzFDu\nvB2TMiowXmdHn4cSFovymp1VKxCTxQyFeSlw6DaFNyExU77ZjPsfIDlrfpyWCSiMsTnGWNvyN8cY\na2cdtycaNR6GaDhAkT7M3Cgv6bIQGkoFDgLU4VkBJTEq0jSUJ+I3AOS0jIwor1iU74axy2sK82ok\npSmWoR/hfm9vkcwRxRnKdKUbgy8BCmkow7q8ph3P3JzJUOIOzw1jAEuckyyTofgqmgl1MTNavoBu\n5EkNpWpnKIaGYs5DSTKUZBlBIJoRtStzzQqnEqIedRPlpXJ5+Y76KuehhN0+5jGNJtpSQ6kWYihZ\nGooVULT8XbrLy20It1a/Kwpr9SdSqUoAIOiHoteMC2laxxjUMa+eFpXIcnn1ekJfsAEK5bgahYYy\nWQswNWW8izIbgrP9QUwcICKr4nV2EgwFyYoYoB5FyQUj9Z8JUCZYLwkOjoNOXSCJqaEkGApfZkDh\nnM9wzpuWvxnOeTPruD3RqPHsV9uhKLfV5SVF+WogNBQJKOYyHmThfC8WNn248EMmOjfmi5QohulM\niRiKUwlxCH4LQDIUeivppGaUVy9EKHeJGQqQeNse9PZCyKsxoFjnoVgZSie+V5QCYyFRXl7kYsbt\nmpsLAAovxFBSGgqCJKDMqIgrJ5IaCs/QULrdfA1FYyihIewT49yvtgMhHLQqKswTEAOTGhf3OxXl\nFbu8RDvR56HM9cQ+TbRRlZMyeLc4QzGWGrEDijY7vjmt7qdTEwzFlwlI2/5EaukXQCQQxX4qND0L\nUKhjXj0jnmsCUAyXV0NjKIypW7VgQNE1FOmarrsRLe0SG4WGVxFi4lFrEvXG9HQ6yssylwsQbjk5\ndSX1s55N2Yer6nbggeh64rOpoYSNKXVtyw0opjHG1jLGHkV/46zUSjPqJNc3tg5gKFKUr/rw4SKa\nUknjqlWDoXAOr6elRIGLgACl0ofnpztPfVTk97nwk9c9HIjNYIwrhuK6avRnAErgJRmKDVA29UQi\noQRDGeDyqlf6cHl/sMuriIYSuZh2PXPzQEBp1JIur6Jhwy782M1DDCV2eYU9OAgQhGxBGkoCUFg1\nMbGROqX1kyLd/naWTODUcMP4elIaCrm8fMPl5ftodcU+xFAAIOz2k8cP4fKiOuthw7NaQkgdUFwX\n8FkdfZmLru3V7QzFi4D1WkJxQ5QnixlKU1QmHtDZRHmmGIrjJFJxxfuk6lFQQ+nJaLNGLYrXViOj\nLMkOAjjr94frRKLe09OImKgEYwDnTCREpIoYgELtwQYocRvnXdE2yY+3bl3cFc3MAIxxpaE0VO4z\nPSfaOK1IlNfzGGO/BXA3gKsA3APgyjHXa0VZuy0e7hpnZy6gUJRXrSIAheLTaXXWBKD0esmFqSAS\n4bnw0aj00SsAKG00Mdvw4CLAfqt6SkPRO4sYUCKhoXiKoWQBykZPhAz/nlxVdJDLq9cTo2mX+8UZ\nSoaGEvUD+JGDKTcJOHkaSjyCrvGEyytTQ/GSPzgmQ9E1lLCHKosQhsgGlJyJjcRaGaIUQ6F7tW5a\nROtsZ8n14epVBSjxKnwmoOgMpa7yggGI56EAQNQ1OrEBubz0a8hjKAwRpidNQHHjiZjtfsMKKKEf\nqqnmQCKXl27xAmhN8WO+htJH1VE/UxMlQBl6HoqeyytSDIXWViOLnQIIgfXrMdmIBEPR5qDEbm9b\nCLc0KpOqYGUoXDIUWnVy/fp4EujkJOBWQhU2XFdRg5TeadxWBLbeC+DpAO7gnD8GYqb8NWOt1Qoz\niryYre7KZyjMhfAgJ0V5ApREB9ftJtO+w4UfViVD8WOxVTebhkJ5lNbt1UkyFDKK8nJF5+57XKTU\nIK3GxlB8IfINxVCqARzuxz9naiikNmYwFNKOpmvJEOC8sGEdUHSGEkUGiJPLS8uZBgzQUMIeHBYl\n0oFYAaVWE8shS/HWZCh1eAgNHSYGlBnxYTs3AMVRmlCfbpcZ5eVbAMUXiKAzlBSgLNblJdvdDOZQ\nqamyXFdMoqN73PLsgBL4XGRAnFRag42hkOuoIZe4j98/W5QX66NSYfHPRRhKYZcXAUqNpxmKrLeD\nAFi3DhMNLoBQuya63QlAMZ4BlalrKHXj9Zzg86JtWhjKxIRYMTIOG4602fUZDoFRWxFA8Tnn2wFU\nGGMVzvmPATx5zPVaUUax4c3qfL6GwhyRIA5SQ5EjBKvLq9NJA0pUgYNQAEqQfuETGorPZR4l0VLW\n7z2vNBQdUGgeiluVgBJJQAkzAWWjvy8a1T72lVNRBq6H4hFDUZ15JkMBVKZei9H8Gyug6KPqhgrL\nzAIUwADxTJdXPwUosYYS9FCtJAElpaHIcvlME30zlxexVvSzAWWVmHSxnRsaiu7yMqO88gBFdrq5\nLq+MsGFde8hjKDMyVLiJdqIsxxFtmUbUba+OViud7DDwIV8q2XtOT+dqKG6jitnZAi4vh8U/mwxl\nUaK8ASgJDUVnKOvWYXJS1lsT5ONITz1AIoOh6C4vwttYlA/nky4vjaEQoNB16G1/JQHKTsbYNICf\nATifMfZJABkxn3umUShfszKAoVRcJX6zWhzhY3V5WRhKEFXgMh/1SjCQocQainyx1+01n8lQAjio\n1qpyHopw3zh5DCXYD+undyQ7lgGifMMJ4XJVRqaGQsdmAYpMgjddV7833CCtodDqmzqg1JFweQEG\noMQMRWYS1kAvC1DcoAuniMsLQL+pNJCYochy6vASYcMJDWVvMeni4UhE6xBWJhmKfFVNQJEArAMK\ndbpNtFF1xHGR5yePz2AopHXr12ADFLdewQQ6AlCcJEPx4cYA2PZqaLfVgp/qXFwBiszwm6ehVOuO\nYAZ5Li8oQKH1t4BiGkrD8dMais5Q5P0nUT6Toaxfj4nJSsxQrC6vITQUE1AakQEokqG4riRtVY2h\naL30SgKUnwJYBeA4AN8BcBeAfxhnpVaaEUOZrcyhj7pIDWJ1eTmoMG4FFG31WWGdjgrZhdRQoqqY\ntV0NEiu06fWgNu73xcs1OyUZyl67MDcHtOYdWDWUWlXOQ+EJhhKhikAfNQHYGOyPddM7kx3LIIbi\nhnAjVQYxlFTqFXmvsjSU2OWlAcqEG6QZCvVQ+jyUOlIMJXEaeXwg3TETcslghwe5gEIMxToPhfO4\nXG8mDSikoQiGUrEylPVrRMHbQwEolMCw7kSoIkK1ylVaHjOXVw5DmUULFVdmWO4ZgJKhoRQFFDgO\nZtHCLFqJslxXRBTFgNITory5+nYAOXeH/mBvEl1MwEUfrObaAUVnKPDiqtA7BxRjKBNOkK2hzMzE\nwF2vI19DWbcOk9OVWEMxl6FJzAnKcHnpGoo5hyaO8jIYCgFP7PIyNKmsaV+jtiKAwiBWTvwJgGkA\nF0kX2CPGWrduRHPbnWhWhGvixbgAm7eIxvAr/CE+gHfGgFJFCIf38UC0L15/7UsAAJWLv4FKZw63\nfOcBvP+kHnDcccDDD1tcXlURwlr1ccuux+Id/+mJdb87HXz2s8BFF4nGVqkIH3QbTTSnRIslt8nG\nVhPn9V6E73xHFkyAUndkckjRcThMAAog507cdhvwwhcCRx0lGMrMTitD+eGWP8Dz8U285h2r4pfF\n88Ro2olUZ04MpQKOSoXHDfprXwOOmj8fR138crzsCb9A+4F54Iwz8LNn/TfOeP73FKA01BswWROA\ncvJp09iAx+FqPBNndP8dAap4yxkHx+ts2QDlqquAM84Q/c6rPvZEPB/fxI8u3Jqoo8v7qP9IxJnc\nhYPxX9f8Y/xs3Ft+AadiZyibcCDeitMQVmv4xCeAn4TPUs8zZihSi4GfCD1OuLzWiI0PBQJQqHOo\nuzJBpQvcvHlfvBcnxj3Tl7b+Ha7As+Mkor0e8Mkf/gGuw9OTLi+pb/z0nkfhM3h9DCjv/+2/4Kij\ngLPPFuHLxx0n/lcq6bgJK6C4LppoWxlKwKtxEMGObgO7dqUBJUQVH/jZs3DU3R/H2eErEufTrYNJ\nMShxiKHI3pYqqs9DYZ6VoVDf+7nPAUcdBXzwg6r8GFBc366hSPbkeUAFIRyXxQzlpS8V2U+o3k4V\nwL77YmKSoVudxrdb/w9f+pK6L4BgKNfeeyA+jWMRVGp461uBB0TGe6uGkmIoEPNQbgsfh1NwEviB\n62gVYAC7gcuLc/4ezvmhAN4A4AAAVzHGfjD2mq0ge0zvdjwhuBXPmrwJh9V+g2/iBfjpnWIa+YV4\nEU7ABxAwF1FFhOc+Z///xePdu7Hdm8bTqzfgjy99Fyq+h6s27I8T39dA71OfA378YwugOHBYiOfv\n93PUWR+nfqKO3ifPBK6/Hp/8JLB9u2jEjiNcXvOYwtSEGG3vNyPcJtvmJ/H+1hvwmc/Igg85BOHq\ntajus5cAlL6I+KhWOOpPPQyAHJVfdpno7e+8Ezsqq7H68fvEL6SuGXxp69/hEjwfZ31pAvfcI37v\ndICJfabhTqvr0Zdi1SWTz38e+L7/p7j1gb3xlQ1PxY3n/R/woQ/h3Ksfi5MuPRwdma5cTzg4WQux\nZQtwykcmcTGej3PxCpyy8RXY8DdvwsfO2xuXysxy9Wcdgdr+SWH77LOBk04Cbr8d+MIV++MSPB/n\n3XioKDd2efXROP8cAMA38AKcds2R+O3v/R0AwNlnFZzVs1YN5Zv4J3wUb8Wd3nqccALwORwTnzee\nh8JcOPDjGes6oFB6kf32Fc9wuy96EupEGq5cZsDluPKOx+JdeC/mmRhuf3D7q3E6jkVfc3mdcNEf\n4ly8Am00wRBhCvOo1sQw/dzbn4ET8b4YUD50+1G49FLgk58Err4a+NSngGuvTWoPuQzFdfEKnIsX\n4cK0hsId9GWY6l1bRWAKZV2I7w8cfPi7T8KlDz0Tp+NYAJaoPADd2izcagQ89alCuzjoMOAlLxF+\nQenyinONaQxFF+VXrQJe8AKx7dprgQ99SJVPQZHuVB3+Y5+olmEEgH/4B+C1r43vQd0Jgb//e/zV\nXwFPeAJw/vnATTeJejPGUTn+TUClIjSUtQfh0w8djVNPVfUBBKB88fan4QS8H3ds2wsf/Sjw7W+L\n36g9EAC+/OXAi16UfAYT6CKAi29Mvxwn4xS0J/ZNMpSZBvrrDgYe+9gV6/Ii2wrgQQDbAawdsO8e\nZd94zFtx2hPPxWGNDbj4UccDUA+Y/NUea8QM5YX7X43/Pej5uOXfPoHr2DNxiP+bOHwTkLR9y5a0\nhoIqXObj2Md+ByesPVvt227D84DnPU90ANRBe6jHLznNXG73amhFM4qST0wgfNwT4cxOCg0lIIYS\nofEfLxV1Rx3YKtcKveEG9J0p1J5wsNXl1fLVbG4SA1stYPZRs3CPeAoAEUpK7AeAWBZW65yOqP0S\nX8W/ivreuwO4/354EKv/tSLRYe4zo2I8J2qhWJ5X1tVDHT6rwT/1YwCAbSJtGBpHPgXuHzwh8ezm\n58XITxdRt0OAzoTOUAIJyBBD6V3/IkbN7rcvQXX9gVaGQssbPxytQqcDbGuo6Vmxy4s5cBCgijDh\n8ooicf9cF6jP1NBAF9v7M/TIAAh/PSAAhazdqwETE2iF03gIysXW6QDzPUdyhqaIvgKPXV5eUMEc\nZsAdFyEq2BWIk+j3ptPJd3np81DgungHTsW/44sphsJRQVeuJHrrZvF+HHJI4rEIcKU5NLOiO7G6\nvGb2hbtqGvj93xeupnAK+MpXRPSAdHkFgfA8NuBZRflaDfj614FbbhH4MDen9EyKYXGn6wiOODI5\ne/+5zwU+/OH4HjRmasBzn4s/+RPgvPPUvRELkTHgIyLt4cQE0N3nUWjV18bviO7y8vwKdmEaO3rq\nGVBZlYqq91vfCrzqVeKzzlAAYNcaEYLZ3lVJMBR3qob+4UeKYJyVyFAYY69jjP0EwA8B7APg1Zzz\nJ42jMoyx0xhjtzPGfsUYu5gxtipjv3sYY79mjN3CGLtxHHVJmOOIXjgMUacJcDRxC2r+QShTnMSp\nWTWtgKJt4mMefDA9DwUuXBbG7oR4XwkoBB6uC/Q8IfjWG+IF0AGlHU2nRMOqKzWUgBhKlNQNtm0D\nqlXwxgT6ffES2lxe+rre9LLEazhI9/yE40OPetcZiogI89EMHxbH3n4/EIbwJvdCABdbIELLaGY0\nAEzU1ZK9MaAELC6TsLBeVxIB2fy8eARbtqhtJJTHOg/3UfeFy3CrHCtRtlsSO21hw5sg1mKhMGuq\nR7wPgEBOKq0iTLi8qG71OoDJScyihUDOZjYBxdVc7e02gKYAja3auI5AtY0mWpgV2gYQMxTPF9nc\nOphMrPRJK/ACgwHF1FBsn+lYis7a1hYvzGMfi4T5cJNzaGBnKCQ4Q1x2MgTZdYEogifzhtV5L842\nrDMUXftuNgX4EBsghpITJwIgvWKlHs4bhsnbMTkp6t1uK6lVd3l5PgNHBfe3xD2iazLPAahIeV1D\nAYBdXSc+NsFQlKy0YjWURwM4nnN+KOf8ZM75b8ZYn+8D+AMJWHcAeGfOvn/OOX8y5/zwMdZHWLUK\nWpS6Xpc+a+m7JkDp8TpCVk0CitbK9CkUNkDx4UpACaTguVPt224nGpvrAvNdGXUiAYVmLm/rTsPj\n9ZRoWHXFpEk/YAhCJjQUHVC2bhUiolxVLgUo8u1sB1OYhHgb6WWJU2674tjJalL9pA4ZoJxLQdzh\ntTcIB3JvSoy2qZPee0bTUOqqp+mhgR4aCKJKXCZ1pjqg0GiaOg7SWQ7AZlUuFz+6vI+6J25YzFA0\nQKGAiiyGQpmZqR7xPhDpVnSGok+u27VLPtOJiXgAAWgaSs3CUNpAOLMK85iO66qfmxgKlVepi4dI\n0XPtaDpuswcckGQoFAFtaih60ENsei9tiPKAlnpE2sEHq88Nx0cPYulrvfwgMFgQVGYbQAEKJY6k\nh+3NiR60Dg9VtxJXiY7Tq0qCd7xQnQyKzIlkF+fw7IDS6xFDUb9NTIh6U2cPJF1elH9t08NTibqY\n56DrACwMpVeNj01oKDWt7WkTVVcMQ+Gcv4NzfstSVIZz/j3OOfUkPwdk77LcRgwlilCfkKMqirMn\nhsJrIsqLGwxFGlHx+JgMQHGIocgsszaG4jhpQCGGsrkj5jKkGIrD4FYi+AFDGDE4JkPpdBKp4Glk\nDiQ1lHY0hX0hhvs6Q2k2Abcm6jJRSQKKjaHQWuntB0Wn7k0KMkqAojOUyYZidzaGQsCm33ITUDZt\nEv9pnRbAEOXlSnsdTCWOo9GrTUOJGYq3JlGPeB8kGUqkubwADVAmJxOAYjIUp5oElLmp/RJ11c+d\nAhQpylMYug4o69YJ18+DD6oyhtFQYtMGTvRxXqtbrRYvrAlArP2u/64zFBNQ9FMRuyCwjwFlXtzs\nOu8lRHldTyEjwXtUgJLHUHQ3a8LlJcF908NJhjIIUHRX8rwGKDpD0eIUVEj0SgKUZbRXIjvFCwfw\nPcbYTYyxYzL2GZ0RQwlDBSiSocQaSpbLS1rFUbc6C1ACOHArgQSUnar8Vivl8trVFQ2K6lOv+KjV\ngI1dMdJvtdRILg4HdTiCUDCUKjMABUgASq0mWFWlYmgo0Qz2k4DS6YjG2+vRGg6SoVSSOS7SgBKg\njj7q6Kn7VxdvOo36V88qhjJRSwNKoAEKmc5QqFOmjpYYig4o8eTLSAEKmclQTJdXWHGxGSJ1yMau\nEcIEnaFUFUPhaUBpNJBgKJWKes51yUx2tlXbabWA1uT+iXPpfXsLswJQmLieap0ARfxvBVPxPadO\nnu5Nt5uM8ioMKAMYyrp1SaCYqPrx746TBGpzzXe9TJNd0A/eLlFRHVCyGAoBCnX2sYaSHckuzpED\nKDaGMj+fXG5Cubyq/7+9c4+SpKjz/edXWdVV3T2TNQ8eMzDDG3HR6wPb8QG6IIiIKCC44AEEH7Dq\nIoqurlxfl6v3XF+7q967ymHXFXfXRV1cjx5EXVS8iivgKIKooCyijAPOIMz0zPS7K+4fEZEZmZWZ\nldVd3V09E99z+nRVVmZkRD7iG9/f7xe/iMj9oUcbibp0IhQ9aVq3dfdYEB2bVihpQhkc3IsJRUS+\nJSL3ZPyd6ezzLvTkyc/lFHO8Uuo44MXAX4jI8wvOd5mIbBaRzdtdm0Q3sEPUVotqo6rXK5lKmry0\nQgkIrEJpNBLDlsAhlJ00YWoqNQ+llvShzD4elT+7Y1di9OYSSmMoXmMhDOEhk9hxejo58rMv2PRs\nJVuhAIRh9ODZjjkiA6tQCDmwop0FVtaDftmrVqFIJ0LRWjxkVF+/RoPJiu5gHmIjQaVFuDIelQ82\n4s+WUOz5XQwMFJu8gkCxjkficiMfyhT1md1uUW0+lNnZ5DyUbTsGIl+MJXEXkQ9FuT6UfIViTYBu\nuGt9wER/7Yh7q9FRGG0kY2LcSNfIh2KyNccKRdd1dHY4oVDstbHotQ/Fnsc9thFMRQql2UwSSieF\nAg6hlFQoWYTiKpQyPpR0Zx8lE81RKOmyIkIJalEWjIe2NRJ1sd2GCzdTebXSiuZ17TYWiiwfStrk\ntZiE0j57boGhlDql6HcRuRg4AzhZKaWy9lFKbTX/t4nIl4FN6AmYWfteC1wLMDIyklleRwSBpv1W\nC2nUE+uVRITSqtEiIGAm9q65CmXA6RTsMdLQWguYrtSZbjk+lFZMKJM7dOeQVCj61tUHTbktnbRu\ny0PxaHl0VD+g7oS16fGAmZYJG7Z2YEtszWbC5GX/W4UyRY0JBllX2Q6t2PEI1uSlr8mQjCcOdn0o\nk5PQGNBPd5Od+lps3Mhky9iV2UDYmKJS13nRZqkmTV7SiJZNdgllYCCZNiStULZs0VlxV+3cER3j\nzuZ3o9KgWKHMzsLv/hD3LlvG15BGRCitfIUSOeUdhWJHyxATiovRURitJxVRGBJFwY0S0mBCm0Cn\nYh/KhEkdMjo9yLR5/myy3y2xaMv0oUxO6g4zkUqtg0KZdAZLGzcmgyUGK1OM2cmXTV33Vktf1yyF\n4vpQ7DUAokIndhtCaY2XVii9NHnNzCQJpUhlzQYDkVrcsn0gUZcihQJphRITSiLKK8fk1U9O+UWD\niJwG/BXwMqXUWM4+wyKy0n4GTgXuWdCK2R5xdhbqhlBmUoSiBqI08YyPtxNKLYNQas4COJW6VigV\no1BmHov2nXxcXwrXh2KdcvUhcwtbOiXE1ql4tGwfVDuCqtUU060MhVKJE/S5Ji+36VSrUXTQukAr\nPVehhCHU6rougyTznbcplGpKoWzYEL1kWzlIp5OpVqOXZ2jQVSiNTIVi25JWKPa8W7dCuEIlnd8O\noVRQ1JxJkZZQ7OjVdcoDPLgl7kG2jidzcIGTy6slBLT0PJRWZx+KSyiNenpFNmPyqiUVkTu5u0XA\nNg6I5vEEDV3YlCGUnVONyORlCWXr1vj4PB9KuqPr5EMBnYAUtEIRiYMMGpVJpjHzjcL4HGUVSuSb\nsAplLFYo1rScp1B67ZTPMnkNJeMRovqAWbXTDJ62bq8l6pJ1nV0Sr1YcQtmTr1DsO9yXTvlFxv8F\nVgI3m5DgawBE5CARucnscyBwq4jcBdwBfE0p9Y3s4noExylPrWYUSsAkA9FIbFLVYkKxPYXzJM9I\n/DkilGrsmJyp7F+z+gAAIABJREFUDGinvCGU+sweBoIZdtJkclT3QgmFMmEUypB5Uo3Jq0X8ZLt2\nYj3yFGZaATOtCkFFxbLd1iODUFyFYjuidVW9fke7QjE+FBMFZitcG5Bsk1d1TJe5cWPkqGwREA7p\nt9y+PIMJhRKbvFwnuG1LmlAsWi0IVyYJJZ3A0lUpu3fHPqS0Ux7gNw/p676Kx2mp9tcoUigzOitB\nockrV6EkyxysTGiFEiQJzCUUsNfQhKvbKC+zQNTo1GCbycvNMZdn8mrr6DuYvADWomWTJS6ruBoS\nXwSXUDo55XN9KHv089RojWXOlM9yymf5UDoRilu3alWTZJbJq6NCMeTeMhGVRT4UkXhqTFBRsclr\nLD42L8prKZzyi27yKoJS6qic7VuB083nB4CnLma9XKc8lYpRKEGcVwiYbA0wS4WAybincJ7o3ZMZ\nhFJzVlSTAeOUn42e7rA+xehY2IFQYpOXa0sH+NnP9BywqSkz8qwJ00oTStUxeU3WhmGKYh+KCKOy\nChTsX30cET0ychPaVa1CseIypVBaLf3fphQJQ/jtY0ahOEseN4f1W25fHvdFdhWKSyhphZL1UqcJ\nxYZg2vPUmWS3UWG7d8edQDpsGOA3v63QYJwjeICf8Iy2c7l27KDSIpidpaVkTj6U6LpU9zA62mC0\nkpyelb7vEK/zbp3yE8ZMODrVYNrMpF+/vp0IbQ4skS4USobJC2A/HmUbB0bEZdsTKVin7jb8Nk2O\nbpm5PpTxeB6KGzacpVDsRPgsH0o3TnkR/T3PKZ/XhlalykQreTGL5qHYtszMpBWKSW3zuH6/ixTK\n4GAyQGAh0W8KpT/hOOUJAuoyxcRMNUkos1VmqegJjNY47gytbFQGOJFhwTCNhnl5jUKpyWx0vrA+\nwSghE6Mmzt6dhzKly64PWy3dansZr7sOPvYxnSsoCIxSUDVmWgFB4BBKMBzVM9eHQjwybq6YZXAw\nqVCaTajVdRuHWkahGCaoViUxwq9X9ZC4uabK6IqD4ayzmJiIw6rDodlIoQTMJOzvE9KIfD5ZJq90\n2LCLZpN4wh8zUd6vLIWyZ09clqtQbFqMR/4grOWPUXkuhofzFIokyoh8KGFIePxTovqnfSjXvfcB\nPnrQ3xDWxrVCIckgWYQSrtTXuNLQF2/KmJhGJ+s6Cqw+xaqMacNuJ+z6UMqavNzNJ3Ar5/zpo5xw\ngmmPKaORQShphWJzP7rF5xLKmG5rfTZboaS5z10Tfq4mL9ueiYlsp3wakcmrUo3UooWduZ95nYnv\nSVCJF5CzwSY2/ZG9l96Hshxgh6itllYoMsXkbJJQJmYdk5dSSUIJQ8bGU/NQgMlgiEbDPMyS9KGg\nFM0BTSjRxC3Hh2InhdUHK5qRjMnLxS+cKaiWUACmVDWpUApMXu7IzY6Mm+uHEpO3zKGxD8UllCCI\nXtaIUMz8inBlSzuYn/nMxMg9XDEb+VBqTEf1hvworzwfioswJDYtmbLtZ0gSilJxJ2BHiFNTceba\n0VEYlrGE4rFYudLxoZjonIDZyIdiR8n2MaFSIXyjToOTIBTTpotP+wNvW/VpwoEJ7UNprUycL5NQ\nQn3NbJTXrDFG7JwY0MseNCYTaassrM3ejXrqRqG4Het6HuaG/3Ufa016tcjk5eR5S/tQajVdB/te\nuKey9Y18KNbkZWfKt8ajzAB5TnlIZgsuSyhZ6mEuCmU2GEhMF7DYvbszoVQdQrHvv33HrVnR+1CW\nA1ynfBDQqGhCiRb7QSuUFpU4Z1eKUCyEVkwolcFot0lpoKhQrbSi48Ka9jHYOHs3bNiiPliJFlux\npxGT5sWNkg6CuMOfaA1QTSiU2Cmfa/ICdla0Qgk3hIn0EraJtvyh2V3xNahUCgglni+TIJThVqRQ\nNKHEj6lLKEUmr0xCaZYnFNt20LffnssllEGZiMpzU0ClFUpQUZFCmZiIFYpbT3cNjDShWHtbOGB8\nKLPD0T12j3XrEK4ypp9a8hUfHR+IVvqs1dpH065CsW3INMWU8KEMMJXoaSOFomJCsarajZaqVpOv\nj3sfhoayTF6OQqm1O+VTWeITC2R140NJX4NGIz9sOA3X5OUSir1nO3eWIJQgDhu2sO+4SyhpH8pe\nPQ9lWaJNoUwz2aq1m7zMnAMgOQ/FsUWt52FGa3rINlEZilwtdiGhSKGgV4gcJYwmQqXNOmB8KKZ+\ntmNZP9Q+aq5WoWpMUuOqkTB5TZQ1eSk9RAwPXZ1QKEGgH1pb/uCsCZHqRChNbQobG0tK8ubKVuRD\nqTJD1WnvBI0oQiiLUNJhwy6aTYl9FaZs+xnyCSUIYhODJZSdO/UETlveemeuYZvJK1Io2uRly3Dr\nbR+RhA8lRSjN+qQmlJnBxHya6L47dWiu1q+2O6EWYHRcK+umWcI3rW6yCGWuPpQa04kwJVvGYCu+\ncS6huOHtrgsyHaXVFjY8rpWPa/IqUihuTrC5+lBse+biQ3EJxd6z0dHuTF5pWD9VlsmrXveE0l9w\nfSiVil6iV6UJJTBO+WKFsoEt0Uh/stKIdhsXh1DMmxBWDKGYBzCLUBpDsUKxL+f+Q7vbkiQmFUo9\nqVAqg1E9c6O8gNFJE+p55P6J9BLNppkDYqO8WkahOCavhA/FmD5sp5eebxquVLkKZZeKe+OuTV6r\nZM4KxU5qdAllsDIZlee+0OnJZUFFmbDhfEIppVAGpzShTA2ylj8yaOb72GPd9CZh05i8nHB1gNGx\nqlk6ujOhzNeH0q5QdJ0aDqGkTV46vD1bodj926K8rEJpjSMmk2Ze2HC6jPn6ULLmoRQplCmpJyIx\n7T2zhJL13EZmyCCbUGq1eL05a/JSKknQ3ofST0iZvOqVaZ1u3SWUmZRCKSAUO9KflJhQorWzg9jk\n1azsyiQU9+GtDwVtJq+wPpnZUdQa+kGeoB5FwQRBklDSJq+ED4WQgBkGD1+XUChuZwjOWihGoVh7\nfJQxta4JJVyt65NJKDk+lNFWNqGkw4Yzo7xWVaIcYlmE4jqLbdvd/5A0eQ1VYpPXwQfHTXZf4JkZ\n3REEzDIzKwk/jN0fsgmlYdsQEcq09qFMDOiMXVXdMduBhK0DxIQSDCQJZeeeamLp6DxC6YUPJU0o\n9h5FCpYkocTzpZKvTzrst20eisn8XZ/ZA0EQqZOyCmUuYcMQE0qZsGH7+5gMJ7anCaVQoQRkEsrB\nB8ekY5//2dmkCdErlH5C2uQVaEJJ+lACZlVnhbKRh/SaDhCtZ1KrwZgyuY1ckxej2odSoFDqw9U2\nk1fYmOpAKA29uhzGDlyJDflFCsWmRZeNGxI+lDShDDEWr1eR5ZQ3CiXcT5+kjVBCkgql7vpQ4re6\nyOSV7UOpMMA0Dca7UiiuOcP6P6am9IxvSyj77afPGQVZJBQKBLSiSDbXh1KsUOK0OppQZnR24D1m\ntcQgSShr1xKrljyFsicwCmU6caxFnslrLvNQ2kxelvRn8xWKJYNSCsUSyqRx9rf2QLWaGCylq2rb\n3I0PxRonypq8ihRKmlCs76OcD0W1+VAgqUwjJTSVJGhPKP2EtMkrmIkUSlVmdG6vaU0o0bonrhHY\neWsPYBtTs1UzKbLu+FDMnA1HoYRqB9MMRMSVSSgZTvlmYyqzo6gO6vqMMxg9pPU6TBpzW0cfis1i\nu3FjpkKxzR1kPO4ZHB9KlAbdEsr+ukF2HZFGMBVfroQPxZhKUp1aWae8/dxs6ko22ZntQ6lkBz9k\nKRTQafqtD8UujW7vZ8KHEiiCSitSVEUKpdCHMjyjswM/Guj13GvasWOjn5pNaAa7EVqsCHOc8oZQ\nmsPTifPaNtv+vysfSlmTV0OoMUV1pn1ioxt+W2TyyvKhTNrnisk5KZROPpTMXGZ0VijuM5gmFBvp\nZgmlnA8lqVBs+bYMiJ//qSl3QrMnlP6CVSiRyWuGCRr6xRwYjyY6tpCOCiVaB4SQSTUQ+1CUIZRK\nK/ahmBT2diGlTEJpSOxDMXMPwsHpHIViJrnRSHRakxL3aoU+FEsoq1e3+VDceg3hrNRkCCXhQ7GO\n6HX6zbOEsv+wZoiwKZkKxdqJLYp8KK7Zwa5nHob6QoTVsWyFYhSB3T9LobhkMFiNFUoY6j97PxMK\nJYBAVESAbriue08HB1MKxSxNYJk4HNbP1pYtQhiMEVbHqdWSkWJhdYywshtp6ILTCmXHrip7WBHN\npLfPiW3znHwoOU75AaZSCkX03KIMQnFH+kVO+UwfyqRO/BnQylQoWYRi536UMXkVEUreeigQX1OI\n22KDb/ZHZ5uw6uKxx6LMTm2IlVbSh2LLdxWKSyguQXsfSj8hrVCqsUIJByZpMMHkdKWUyct2QDtp\nJghlTLVHeTVbOp+XXUgp7UMRWvqzNXnZTsIhFDd6qDaky1VUkgoFE5HWaHT0oYSMgkhnH4rNgx4E\nbT6UaFS+zqzqZ0xeB6x0zDWuD6WAUNx2uPV2R4f2uDDUDQoHJrIJZVCoMMuaNcnr7I4+XXPVUJBP\nKG0+lEpMKFkmL1tGJqGYA8MVTqhwfZJwYJx6Palywto4YWV3tNEllMAxl4RDcbYC9xrNyYeSk8sr\nW6FMU3U6RUuuRU75jj4Ut36GUDopFLtqo0sodinhNLpVKLYN7vNq62CDbw6o6If+oIP0dneRuDTy\nfCi2/CyFMj2dNCHOzma3rdfwhFIGKYXScExezcZElCwyQSjuzKwMQnkV/8QvH90/Nnm1zKzyQMUm\nr+kkoTQuPBfe//64w2FSx7FXKvDII4RvfZ0+bmiGMNS2XDt6CQKoDba/+PW6dtIThiBSaPLaSTOq\nf0cfSkqhZBLKet2zRoTSNJFHq4NSCmVsTNej0Sj2oSQIJQgIG9M5hBIQyq42k1e+QpnOJJRqVY8Q\n3/AG+NWvIKjqSC97bd0y3Hp2RSiD04S1iWxCqY5FG12n/H6Vx+Ljh4sJpaPJy+1BHRVS6EOpG4Vi\n3pFqNSbXrLDhIh/Ki14Ep1+4hh9zHJMPbKHeGo8aUEahgCYm60OxzZmZgXe+E+6+G26+GU47Dc47\nz9Y/WY6dh5JWKKCfyyxCscE3BwQ6z9nq1fp5KEMo1YCED8WWn+VDueQS/exZcoPFMXtVO+/ikRiu\nVSrUq7ORUz5cP8zjE4qJSWG2lVIoRx8NF1wAJ53EZz8Lo3f9hk07n8LJDyr23LOOJx8ww7nn1vnE\nJ2CniV4arE47hKIfuu3rnwoPQ/0Xd8LPvkvtwvfoUzAJDOon7vbbWfv7rfz52pdx+oV/wlNX6XW8\nv26WKAsCqA7VEk0C/UKPhYfBq98MUGjy2nPA4QwfrsltcFDP7p2a0g5pgGc+E85fcSNP332nPuE5\n58Bhh1F7LEUoJz0XGlcQHqhHa/ZlOuPPBtn/n2/l8Gc/Cx57hAv5F3axkmc+42rOPx+e/GS4yaYI\nJV5v/Ior4MQTnTqcD89w0mudcYY2Dxx+OHDFFbz60T2MXvM5nsmPeOVRP+Kp998FwLkX1Dn0jp/y\nndaJUdsBXvhC3bkMD8Nxx8XlDtWmeSL3csHIfbzgBcdQrepO6nvf02a8a66Bww6Dc86p8N3vHAN3\n6uNc/5bbgbzxjbBuHWzaBBdfDIdubMUNBUaO3skpp2gzy4tGDuCxesARg/o+X3ghnHwy1F4xxo4/\n7oouROWYo6PyTz5mC7/fbz/kV/fyvAsPBeBlL4MdO+JRfxah7NmT4Wh22daZUVnkQzn3XNh43aeo\nzs5EbY9C1x2F8rrX6Wfqk59sL/PFL4ZbbtGkctttNZ7FGez5zTaGMGRZrXL55XD88bBmjb4P6U7a\nTX0zNaXLt+175BH40If08/273+lzHXecfr6e+9xkOVahBEG7f+/Nb4Zjjonfv0ih/LdNsBVe8rxd\nrD1IdxHNZjGhuNkLXIXyvOdpcny+sxrUc56j2/6tb+nvGzbAIYfourtJQBcKnlDKwGpGSBDKKCEH\nbwipTwR6hKVSPpRGQ2dnBF71KoDDgfej7/URUfGf+hQ8NmPWh6iNxT6UKW1n3Xbw0zWhXHw+fOwj\n1KoKEOoyFdWJsTEqKK751lHwtCfxdHRH+v3v6120QonfTDsBLAxhtHUgvPe9AG0mr8RIdWgN9aO1\nPWhoKPZhWMm9di1cv/Ed8MtRCNbA6afD6adTe0fKh/LMp8D5H2cAfYmsD+VJJx7AG99nhl27alyE\nvnbs936uvx7+/u+Tt8WuN/6BD8Tb1q6F669PrvFx7LG6swbg/e/nop074RrdW/3rSz4HH9cq4/T/\n+WxOr9f5walx20G/jLfeqj9v3hyXO1ibpsEk//Ka78Chx3DRRXr7D3+obeIA73sfvOqSp/L9S4kI\nxaYigWQH8pa3xJ+vuw641/QkhlAO3L/FzTfbPc4H9ApzAP/8z/r/RdccHxdy/fVU7oi/Pvmi4/jc\nVQDHRts2bdJ/r9Pits2HMjurO962pI1Z8o1iQjntNDht4IN8fPzSqO0uoViFcsUVetu117aX6d6L\nFSsUo3v0ksdRTrUg4Oqr4/1HRmiDe06bBcCqFvvcjI7qv6OO0vczC26UV/r6XH21s1Sx04bxkefD\nN+GYq17OX5yit4Vh/A4UmrxShLJuHVx1VXLfJzxBD7pcU/dFFxE9mwsNb/IqA/elCQLq1VlaBDzG\nGsJQYltqKxXlVRK1GswoQyIDE7FCmdTDlu3b9SCwumqFXrBKTO4il1DGnbkfDuxoLAigNhzPdnQT\n7rnrz6dNXq4PxTV9uE5vV3InEmA5m7JMXvb8maOzjLDU9CUdH283Z2Qd3nYr3A3uDFDzOWu+j4V7\nvqHqdOaOrg/FdlTuI5RHKG2oJAmlm2fKwj1v3rVyi077UGyW2rZ8YVkOjtTXtMnL7hBF1eUolA6n\niBCG2q8X+faKdnbgntOGRKcJZefOpDk3r5zJyfz93NsVzUPJuJW574CBe09cQsl7HFyTaonL0VN4\nQimDlL3Ypg7Zzv40V7mEklIocyjeJZTmpF67fft2XZw09VNbm9VRP9G6EkHgxOSWIxRXoex0EuZa\nQnEdmlnzEVwTiOsUzBq52g42q4q5L1NGFFHPCCWDRKIlH539s8pOhIcOzCbql7XPvAjFHmR7oazJ\nNR3g9udlCCUdNuwuT5BAxsAhfY60QrH7u4Ri95+Y0CaZLELJq3cYCjtlVcK313a+DLjLQ9sEnbZ9\ndjnk0VHd9k6EMj6uSTdrv2o1tgZGPpQOhJJ1i2OnvCR8KHnPTqUSBzuUuBw9hSeUMnDvikMoYwwT\nNmNCaaVNXiWRiLOvOwrFvCRj1sdqntpay6yPIrFfJ0LqiXQJpToc18nO7UjE9aM7EadvzXXO5iqU\nHEKBOB9W2hGdOQDPiCLKuqR5nY17y9qOsxFo7o/OTkWEklAoAzOJ+mXt01NCmadCKRqtphWKHQS4\nyxMkkCMfisKG7f72HanX43VF7LORpajyCUXnl0uYvLpQKLZt9XrcPpdQyigUO0jKWsfFts1tQ9at\nbDaLb3GkUGrlFAq0zw1bLHhCKQP3rhiTl4WN7Mn0oZREogMamIg7UKYYECd1vXlqa7NmjfmKY/Ky\nyFEo09P5CmV0NA4pnJpKDuDzCMUqlKEhHanS1piMyJ89yYUcgXwHdRmFAvkvTKFCcTfOg1AihVLQ\nqdr2LRWhdKtQ0k55N5t0AiUIpcZ0R4Vi/9smZk3Az6t3s5lh8ioxJM8ilDyTVxZRpMuBfOJJm0+z\nLNPuscU+lHIKBbKfu8WAJ5QySCsUZyW9ZtNJYz1Hk1c0EmNKk5XzBoW18bg48+RVp/WwyM7sLhqO\nuxEttZWxNLAKJQyT/g0b9RLt50zBmZlpVyh2vfDEAak62U3WSZk1goeUuEqReNvvtJflwr0kmZYi\n25CMiSv2Y0cfilUoBWaftEKpVAranMYiEoq7sBXEPpSOhJJqe6USPw+ZJq+UQrH/ixTKQvlQigil\nrEJx65KFdAi6JZS0Ss8q0yKO8hIEMlcyTcMrFAMR+R8i8nuzpvxPReT0nP1OE5H7ROR+EXnnglYq\n1bk1BpJrUcQTnCQ5D6XL4kNGkWqQePObtbG4OGvymtbbMk1eRYSyot3k5cbkQ75CSTvUrUJJ+E/s\nAZBp8rJL3ibW7ch7mUR0GXZh9/amJcpOo7RCsY0tqVASPpR6q31j6muaUMx0n/b0KlnosclrLgql\now8lo8ey58kzeaU7xEYjW6GUMXk9zmp2s7LnhLJjR75vxCKPFLLO1cmHkt7fhWvyAqjKTO6+6TK9\nQtH4W6XU08zfTekfRSQA/g4dNXks8EoROTa9X89QoFAShNJiXgqlyc6kfR8IaxNxcZZQZqzJK0Uo\nbk9l4BKK60OxOZ6sNHZzG2URStqh7iqUzMYUEIqLXJOXPdCdad0rH4q7sUtCSSiURqv9ZM4+lUpM\nvPYWpdPULKbJa0F8KBk9Vqy4s01e3SqUIkJ5hHW6fqQm0hTAnteSZb0eO7G3btX/H3kkPkenciDf\nNJZ+lrJMXoXvAK7JS9/MWmU2d18Lr1C6wybgfqXUA0qpKeDzwJkLdrZ0lFcGoUxMaJPXXMOGwTjh\nK5XE+cIBx+RlfShT+u2rBymTV3r4TxxCuGcPiTTwaYViO460yct2LL1QKNEa6g4KR2fuFOas38l/\nYQpEW3LjPAglT6FE9zOMb4erUNx9Ch+THoQNL7YPxf2pk0LphQ/Fri0yF4XiEkoQ6HfFTv6z/3vl\nQykilPIKRT9MtRIKxZ2HspjoV0K5XETuFpF/FJHVGb8fDDzkfN9iti0M0k75lA8livKyPpQUKXRC\nglCCpMkrHDARXa4PZcqYvCrGhm9f2ownzCqUdIhtUEAorkKxzUg71HMVSoEPZdeuYkJp6ziq1Tkr\nFGsxg2R72gorcMp39KF0UChuZ2R3sSPhrhRKzhyjMujW5JVeU35Up25L5B9LFJahCOx1ywsb7qUP\nJfo8D6d8ehnmvHPklVO0X/pZmo/Jy1oValJeoewTJi8R+ZaI3JPxdybwKeBI4GnAw8BfZxWRsS0z\n9ZmIXCYim0Vk8/b0whtlkTZ5pR6GNpNXly++60NJm7yaDcfkVa/DwAC1Se3drgcpp3CG3+bss3Ve\nn498JOVsN6sgpn0oWSYvaA/5PeYYnR7k9LSHq0uTlz1/hrjKVSh2ROmWnYVqNRkCnYBtSJdRXgkf\nSkO1b3S+uh1FmlC68qFkhceVxHwVip2L0XYNy/hQ3vT6djbPUSg2YKNbH0r0uZ6aQFUAe+tdH0q6\nvKxzpDEXhTI21j7e7FahVPvY5LUkqVeUUqeU2U9E/h64MeOnLYBrbNkAbM0517XAtQAjIyNzy7eZ\nNnnlEEoQzI1QEj6UtEKpO2HD5oQxoaR8KBnnbTTgM5/Rn10+zfOhZJm8IH7h3U7guusKGpMRNlzk\nQ8m8ZLVanPImdW4R/VMnQklbXNoKm4dTfmjQPE45CiWLUOxIf04+lDlMbOzFPJRMs4/N4VXkQ/nb\nD7cP/boIG+6GUJorWzDJnBSK/Z7VzrKE4i5JkLWPSyhd+RFxFYoxeXVBKPuEQimCiKx3vp4N3JOx\n24+Ao0XkcBEZQCc2+uqCVSqdeiXlUIsUyqzoNRnmSCiZPpRGilCaTWqTOh9GI+hs8so6D0DVZKHt\nZPLKI5SOJ8nxoaT7RFehZJaVo1A6mUNsFXLrO0dCsXEPIsTZgAt8KG5dYJ6EskQ+lNxONXV/3M2V\nSk5nNoew4SIfikW4MlstZsHevzyF4l6zMj6UlSvzBy5ZPpQis2/hmvID5Z3y3ocS48Mi8jMRuRs4\nCbgSQEQOEpGbAJRSM8DlwDeBXwJfVEr9fMFqlFIo0frlTFKv64eg1dKdcVBR8ycUV6EMakKJHrQw\npDqeY/LqglDsaMeOrPJMXuk5JB2b1mEeSt7LlDn4rlYT1961UnVy2NrDcwf1aUIpOQ/FnnNwEKSW\nbfZZMELJdAYVY77zUArTj+QQijU1ZsJRKJ3ChgusakDK5NXMJvcs2BnseYSy3hnSlgkbLrOPvfYT\nE/mDKrcuLtp8KIZQysxDWWyFsiQmryIopTLzYiqltgKnO99vAtpCihcEaYViRqZhZTeYZXzBZEut\nqq5NEwkfStrkNWhXE7QbQmqP6jehXp2/QqnXky/X1FQyuVzahzIfhdLJh5JZlk0uRlJMpJNYZmEh\nFAo4HWaOY9reT3d0a6OG7LUt9O9Y2PvaasXD/i4xn3koSsHjj7evQxMhFTThnif3XDkKZdJJS5eu\nbykfyqpKewEFKCKUjRvh979vP0dWGWX3KZoX5SqjLD6MfSi6jdVKq2Pcjw8b7mekfSgJQkk+IJVA\nSvS6SRTNQ2kOpUIEw5DauCGUIE6p31aRDs2o1uNb7+bzyvOh2Kyz8yGU6ek5+FCcSttL45q8OimU\njoTSpVPebh8acnYooVAsIVtCse0ohEjMOHMwd0H381Dc5JCg0/Dnmn0KTF65CiXHKZ9Vx7ImLxFY\nscZZYrQE6vVk2LBbno1cFEkOrrLKcI8r2sdtQ/pWWitB3i2OCMUxeXV6HLzJq5+R45RvBu2EEgQ9\nMHk5+SvaFEqzSW1MvwlRTrGSJq9KhWieTOAsDeumsF9IH0rW8R0VSmrEaRVVz3wocySUwUEy2+oe\nl0Uorsmr1GNS8t7mYT4+FIA//rGDyStHoRSZvLIUivNzW307KZSVK6HS7C69rqty00rDEkqRbyTr\nuKJ9it4Bmx24E6HEJq9Wx8fBO+X7GWmT16AJua2YCYbuC1FhfoSSeqPtUq2uQqmOWZNXdwoFYvur\nHe2YIkv7UDpa8+wBzptYJPcLR2cpH4rdrxsfynxMXkU+lKEhZ4cuFMpiE8pcTV7u3KG5+FCKTF5l\nFUonH4p9dsKQrm087nOc9oUcfHDyex56ZfKyZXRUKDUdEVILOisUb/LqZ6QViiWUIINQArp++dvm\noUAhodT27NDb0oRSwndTM2ayoJ6vULJMXr3woWQdPzCQXBO+rawOCmWpfChFCiVrHkra5FVIdi56\nqFC6DRsfXOkqAAATPklEQVS2yO0wC3woi6FQgkATdIJQulAo6c+2CLvOey8JpegdsGXk3eKK6x6q\nVqlWlFcoyxqpiY3VRpUKs4TV3hBK2zwUZ2NzZSt5jjCM09fXUjO1SykUfUzVIZS0D2UxTV72/GV8\nKPb4nvpQ5kAoZX0orm3dXr/lolDcffvVh2Lr1mzStdMgi1BsEWHolFuijPn6UGwZHRVKVRdUCzqb\nvFasyEztt+DwhFIGqdQrVKvUmaTZY0JJKBRzzjaF0mxGi+zMxeRVDYwPZcCZ6xLCfffBuefqji+L\nUHrhlM87Pnd0Nk8fykISSrc+FGuv78op75a9RD4UmJsPJfdcORMbnZ/b6ltU7zCcn0IJgvgQW0Sz\n6ZRboozFMnkFAYZQOpu8rF9mnw8b7kukFAq1Glfxv/nTNY8A5zEyAi99qQ59PPm4w+BZx3RV/Atf\nCG949k847LYH20xe+62e5cor4SUvMTuHIU/gV1zKtZx00H3J+pVRKMMDMJb0oZx3Htx9N3zpS/q7\nSyg2Z9eOHSVPkTEP5cgj4RWv0BFDZ5/dfsiVV8K6dRllvfa1MZMZXHGF3vdDHzLtKehsLr8cDjww\n58ezztIOo9Wr4W1vgzPOiH56whPg0kvhxBOzD33Tm0y5x50Cr389HHJI4vdNm+DVr9b/LT75Sfjg\nB+Gkk/T317wmuVJmLt7wBrj9dl3fOaAsoaxcCX/5l/Fz9rznwYtfrD8///k5B11+Oey/f9vm17xG\nhxtnolrlCB7gz592Gyef/GxAp+/5wQ+06nvKU+JdTz5ZX97DDsuv95vfDPvtB2x6EVx2WfHODrLI\n7DnP0fftWc/S5boLoWVheBje/vbiW3PmmXqQtmqVrt7998MFF7Tvd+mlOgAiCwmF8va38+oJ4dH1\n2fu6uOqq5DO4KFBK7TN/z3jGM9Sc8KMfKaXD8pX6+teVuuEG/XnTprmVl4WPflSXedVV+vshh+jv\nn/50cr+vfCWuy8UX621nnqm/X355x9Mceqje9cYbk9v/8z/jYt/61nj7nXfqbSMj+v/kZIcTfPzj\nesezz+5Yl/nghBP0ad73vgU9zbJHqxXf18ceW+raKKXOO09X5t3vXtJqnHaarsbq1UtajVJ4/et1\nXT/xiaWrA7BZlehjvcmrDFJO+TxTx7xgy0pPBEgPK119nY6oKqNQckxFbtZgV6HY023bll2d3BMs\nsNYuYw7xSE6c7ItrVSY8bxGQpVD6FenIu36GJ5QySJu8MkJj5410mXkxk1k50bsweeUVu359fOo8\nQqnXO8zshrij6OW1KTjNcnjJlhrpMcqSolMs8CLBE8rCwBNKGaSd8gupUNJe0QVSKOmqV6vGFp06\npY31z8pBlIkMH8pCoE8GussCZVL9Lxr6pDJuHrF+RyJsuM/hCaUMMpzybdvnizQ5zIVQysxDKRjZ\nWwe2q1BsVJX9XPoE3uTVN7CJFxZYNJZDn4wEvEJZGPTDI9b/WEwfSjcKZQ4mr6KqZxEKdMi31c0J\neghPKOWRyje6tOgThbIcCcUrlL0FqdQrC2LWKetDcSXDHExeRSZsSyh5HNaPhLIcRm1LjVS+0aWF\n96F0Da9Q9jb0U5QXtE/i6oEPBWJCsbO606cr9fJ5H0rfoVOq80WFVyhdwyuUvQ2L4ZRPd8RlCCV9\nzDzChiEmFBsinD5dPyoUTyid0Vcmrz4ZCSxHQumbQUEBPKGUQZ5TfiHDhot6TOvU6LFCsbPV0zO4\n5+RDWaSw4b7pKPsYfWXy8gqlayynKK9lwHl9gLTJayHMOmlyKLI1z0OhFBV73nlw223wnvdkn66f\nTF7eh1IefUUo3ofSNZaTQlkGVewDpJ3y/Wby6lHYcL2uc07lna5UzP4imbz6xHKyLODGkSw5+uTG\nLad5KJ5Q5ggR+QJgMyuuAnYopZ6Wsd+DwC5gFphRSo0saMX2Ead8HrwPZXmjrxSKN3l1jeXklO8r\nQlFKnWc/i8hfAzsLdj9JKfXowteKxZ3YWEahpNd+6JFTPg9+HsryhnfKt2M5EopXKHOEiAjwZ8AL\nlrouQOZ6KNHnXmEuPpQ5KJS5VL0ffSh9YopfFuhLheJ9KKWxnBRKv0Z5PQ/4g1Lq1zm/K+A/ROTH\nInJZUUEicpmIbBaRzdu3b59bbdyIpaVOvQL5PpQFUije5LW80VfzULxC6Rp9wsGlsOhVFJFvAVnL\nKb1LKfUV8/mVwPUFxRyvlNoqIgcAN4vIvUqp72XtqJS6FrgWYGRkRM254tUqzMwk7Qe9DI0tm3oF\n2hVKj1Kv5GFOhOLDhvsGfWXy8j6UruHDhguglDql6HcRqQIvB55RUMZW83+biHwZ2ARkEkrPYAml\nHxRK2oeywArF+1CWN/rK5OUVStdYTgqlH01epwD3KqW2ZP0oIsMistJ+Bk4F7lnwWrn+Cpu+dSly\neUFPfCgLZvLyPpS+Q1+avLwPpTQ8ocwP55Myd4nIQSJyk/l6IHCriNwF3AF8TSn1jQWvVVYU1lJk\nG4b8iY1dzEOZi8mrn+aheIVSHt7k1Y7lOA/Fm7zmAKXUJRnbtgKnm88PAE9d5GplzxNZaqe8n4ey\n1P3SsoA3ebVjOSoUTyh7E9Id/tVXw/HH9678Y46BK66Ak07S3885J5+0nvQkve/JJ+vvL3+5rl8J\nTXzOOaV3jbB2Lfz3/w5nnlli5zCEd78bXvrS8ieYA047Dd76VjjkkAU9zV6Bt70tXo1zyXHqqXDl\nlXD44UtajaOOgre8RVen3/HCF+q6HnnkUtekM0SpuQc+LTeMjIyozZs3z+3ggw6Chx+G7dv76O30\n8PDwWHiIyI/LZCTpRx9KfyKtUDw8PDw8EvC9Y1ksUvSSh4eHx3KFJ5SySDvAPTw8PDwS8L1jWXiT\nl4eHh0chfO9YFsspds/Dw8NjCeAJpSy8QvHw8PAohO8dy8L7UDw8PDwK4XvHsvBRXh4eHh6F8IRS\nFpZQRJa2Hh4eHh59Ck8oZREEXp14eHh4FMATSllUq95/4uHh4VEA30OWRRB4QvHw8PAogO8hy6Ja\n9SYvDw8PjwJ4QikLr1A8PDw8CuF7yLLwPhQPDw+PQvgesix8lJeHh4dHIZaEUETkFSLycxFpichI\n6rerROR+EblPRF6Uc/zhInK7iPxaRL4gIgMLXmmvUDw8PDwKsVQ95D3Ay4HvuRtF5FjgfOBJwGnA\nJ0UkSxZ8CPhbpdTRwOPAaxe2uninvIeHh0cHLAmhKKV+qZS6L+OnM4HPK6UmlVK/Ae4HNrk7iIgA\nLwBuMJs+C5y1kPUFvFPew8PDowP6rYc8GHjI+b7FbHOxFtihlJop2Kf38CYvDw8Pj0JUF6pgEfkW\nsC7jp3cppb6Sd1jGNjWHfdx6XAZcBnDIIYfk7dYZr3sdHH/83I/38PDw2MuxYISilDplDodtATY6\n3zcAW1P7PAqsEpGqUSlZ+7j1uBa4FmBkZCSXeDriuc/Vfx4eHh4emeg3G85XgfNFpC4ihwNHA3e4\nOyilFHALcK7ZdDGQp3g8PDw8PBYJSxU2fLaIbAGeA3xNRL4JoJT6OfBF4BfAN4C/UErNmmNuEpGD\nTBF/BbxVRO5H+1Q+vdht8PDw8PBIQvSAf9/AyMiI2rx581JXw8PDw2NZQUR+rJQa6bRfv5m8PDw8\nPDyWKTyheHh4eHj0BJ5QPDw8PDx6Ak8oHh4eHh49gScUDw8PD4+eYJ+K8hKR7cBv53j4fuhJlfsS\nfJv3Dfg27zuYa7sPVUrt32mnfYpQ5gMR2VwmbG5vgm/zvgHf5n0HC91ub/Ly8PDw8OgJPKF4eHh4\nePQEnlDK49qlrsASwLd534Bv876DBW2396F4eHh4ePQEXqF4eHh4ePQEnlA6QEROE5H7ROR+EXnn\nUtdnvhCRfxSRbSJyj7NtjYjcLCK/Nv9Xm+0iIp8wbb9bRI5zjrnY7P9rEbl4KdpSBiKyUURuEZFf\nisjPReTNZvte22YAEWmIyB0icpdp99Vm++EicrtpwxdEZMBsr5vv95vfD3PKuspsv09EXrQ0LSoH\nEQlE5E4RudF836vbCyAiD4rIz0TkpyKy2WxbmudbKeX/cv6AAPgv4AhgALgLOHap6zXPNj0fOA64\nx9n2YeCd5vM7gQ+Zz6cDX0evkvls4HazfQ3wgPm/2nxevdRty2nveuA483kl8Cvg2L25zaa+Aqww\nn2vA7aY9XwTON9uvAd5gPr8RuMZ8Ph/4gvl8rHnu68Dh5n0Ilrp9Be1+K/CvwI3m+17dXlPnB4H9\nUtuW5Pn2CqUYm4D7lVIPKKWmgM8DZy5xneYFpdT3gMdSm88EPms+fxY4y9n+T0rjNvRKmeuBFwE3\nK6UeU0o9DtwMnLbwte8eSqmHlVI/MZ93Ab8EDmYvbjPoheiUUrvN15r5U8ALgBvM9nS77fW4AThZ\nRMRs/7xSalIp9RvgfvR70XcQkQ3AS4B/MN+Fvbi9HbAkz7cnlGIcDDzkfN9itu1tOFAp9TDoDhg4\nwGzPa/+yvC7GrPF09Gh9r2+zMf/8FNiG7iD+C9ih9NLZkGxD1D7z+0704nXLqd0fA94BtMz3tezd\n7bVQwH+IyI9F5DKzbUme7wVbU34vgWRs25fC4vLav+yui4isAL4EvEUpNaoHo9m7Zmxblm1WerXT\np4nIKuDLwJ9k7Wb+L+t2i8gZwDal1I9F5ES7OWPXvaK9KRyvlNoqIgcAN4vIvQX7Lmi7vUIpxhZg\no/N9A7B1ieqykPiDkb2Y/9vM9rz2L6vrIiI1NJl8Tin172bzXt1mF0qpHcB30TbzVSJiB5JuG6L2\nmd+baNPocmn38cDLRORBtGn6BWjFsre2N4JSaqv5vw09cNjEEj3fnlCK8SPgaBMpMoB23n11ieu0\nEPgqYKM6Lga+4mx/lYkMeTaw08jnbwKnishqEz1yqtnWdzB28U8Dv1RK/Y3z017bZgAR2d8oE0Rk\nEDgF7T+6BTjX7JZut70e5wLfUdpb+1XgfBMVdThwNHDH4rSiPJRSVymlNiilDkO/p99RSl3AXtpe\nCxEZFpGV9jP6ubyHpXq+lzpCod//0FERv0Lbn9+11PXpQXuuBx4GptGjkteibcffBn5t/q8x+wrw\nd6btPwNGnHJeg3ZY3g+8eqnbVdDeE9DS/W7gp+bv9L25zaauTwHuNO2+B3iv2X4EuoO8H/g3oG62\nN8z3+83vRzhlvctcj/uAFy9120q0/UTiKK+9ur2mfXeZv5/bPmqpnm8/U97Dw8PDoyfwJi8PDw8P\nj57AE4qHh4eHR0/gCcXDw8PDoyfwhOLh4eHh0RN4QvHw8PDw6Ak8oXjs9RCR/yyxz1tEZGiB67FK\nRN7ofD9IRG4oOmae5ztLRI5dqPI9PNLwYcMeHugU4OiY/Ee7OCZQOr1J2f0PQ8+PeHLXFZwDROQ6\nc74FIy0PDxdeoXjs9RCR3eb/iSLyXRG5QUTuFZHPmRnDVwAHAbeIyC1m31NF5Ici8hMR+TeTC8yu\nPfFeEbkVeIWIXCoiPxK97siXrMoRkQNF5Mtm+10i8lzgg8CRotet+IiIHCZmXRrR65d8RvS6FneK\nyElm+yUi8u8i8g2zTsWHc9r4QRH5heg1Lj5qzvcy4CPmfEeav2+YJILfF5EnmmOvE5FrzLZfmbxY\niMiTRK+p8lNT7tELdpM89g4s9UxP/+f/FvoP2G3+n4jOKrsBPZj6IXCC+e1BzJoSwH7A94Bh8/2v\niGeaPwi8wyl7rfP5A8CbzOcvoBNRgl5XpwkcRnIdmug78DbgM+bzE4HfoWdzX4Jem6Jpvv8W2Jhq\n3xr0rG5rcVhl/l8HnOvs923gaPP5Weh0I3a/b5hrcjQ6g0ID+D/ABWafAWBwqe+l/+vvP59t2GNf\nwx1KqS0AolO7Hwbcmtrn2eiFln6gU4ExgCYfiy84n58sIh8AVgEriPMfvQB4FURZf3eaHEl5OAHd\ngaOUuldEfgs8wfz2baXUTlPnXwCHkkw1PgpMAP8gIl8DbkwXbhTWc4F/kzjTct3Z5YtKqRbwaxF5\nAE1qPwTeJXqdkX9XSv26oP4eHp5QPPY5TDqfZ8l+BwS92NArc8rY43y+DjhLKXWXiFyCVkFzQW4+\nfTrUWSk1IyKbgJPRiREvRxOaiwp6bZCn5Zwj7UxVSql/FZHb0YtWfVNEXqeU+k6Hdnjsw/A+FA8P\njV3oJYIBbgOOF5GjAERkSESekHPcSuBh0SnyL3C2fxt4gzk+EJEwdY40vmePN+c6BG3G6gijPppK\nqZuAtwCWNKLzKaVGgd+IyCvMMSIiT3WKeYWIVETkSHTCwftE5AjgAaXUJ9BZap9Spj4e+y48oXh4\naFwLfF1EblFKbUf7Lq4XkbvRBPPEnOPeg14B8mbAXdjozcBJIvIz4MfAk5RSf0Sb0e4RkY+kyvkk\nEJj9vwBcopSapBxWAjeauv4/4Eqz/fPA242T/0g0Yb1WRGxmWnc56/vMsV8HXq+UmgDOA+4xpsEn\nAv9Usj4e+yh82LCHxz4OH17s0St4heLh4eHh0RN4heLh4eHh0RN4heLh4eHh0RN4QvHw8PDw6Ak8\noXh4eHh49ASeUDw8PDw8egJPKB4eHh4ePYEnFA8PDw+PnuD/A8gz6K3OgOH+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22dc2899a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import connect\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import RandomAgent as ra\n",
    "\n",
    "K=200\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.game=connect.Connect(verbose=False)\n",
    "        self.game.reset(first_player='o')\n",
    "    \n",
    "    def choiceRandAction(self):\n",
    "        available=self.game.available_actions\n",
    "        return available[random.randint(0,len(available.tolist())-1)]\n",
    "    \n",
    "    def makeRandAction(self):\n",
    "        action=self.choiceRandAction()\n",
    "        self.game.act(action)\n",
    "\n",
    "    def makeEnvAction(self,isfirst):\n",
    "        if isfirst:\n",
    "            self.game.reset(first_player='o')\n",
    "            self.makeRandAction()\n",
    "            self.game.change_turn()\n",
    "        else:\n",
    "            if self.game.was_winning_move('x'):\n",
    "                self.game.reset(first_player='o')\n",
    "                self.makeRandAction()\n",
    "                self.game.change_turn()\n",
    "            elif self.game.grid_is_full():#means a draw\n",
    "                self.game.reset(first_player='o')\n",
    "                self.makeRandAction()\n",
    "                self.game.change_turn()\n",
    "            else:\n",
    "                self.makeRandAction()\n",
    "                self.game.change_turn()\n",
    "\n",
    "            \n",
    "class MemoryUnit:\n",
    "    def __init__(self,state):\n",
    "        self.state=state\n",
    "        self.actionValues={0:0,1:0,2:0,3:0,4:0}\n",
    "\n",
    "    def getmaxAction(self,available_actions):\n",
    "        maxvalue=-1\n",
    "        maxaction=-1\n",
    "        for action in available_actions:\n",
    "            if self.actionValues[action]>=maxvalue:\n",
    "                maxvalue=self.actionValues[action]\n",
    "                maxaction=action\n",
    "        return maxaction\n",
    "    \n",
    "    def getmaxActionValue(self):\n",
    "        maxvalue=-1\n",
    "        for action in self.actionValues:\n",
    "            if self.actionValues[action]>=maxvalue:\n",
    "                maxvalue=self.actionValues[action]   \n",
    "        return maxvalue\n",
    "\n",
    "    def setActionValue(self,action,value):#if there is the memrory about a state,can use this method to update the value of its action \n",
    "        self.actionValues[action]=value\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self,environment=Environment()):\n",
    "        self.workmemory={}#previous{'state':xxx,'action':yyy]\n",
    "        self.longtermmemory=[]#list of MemoryUnit\n",
    "        self.environment=environment\n",
    "        self.γ=0.7\n",
    "        self.α=0.7\n",
    "        self.ε=0.05\n",
    "        self.resultlist=[]\n",
    "    def searchMemory(self,state):#return a MemoryUnit\n",
    "        result=False\n",
    "        for unit in self.longtermmemory:\n",
    "            if self.stateEq(state,unit.state):\n",
    "                result=unit\n",
    "                break\n",
    "        return result\n",
    "    \n",
    "    def editMemory(self,MemoryUnit,action,value):\n",
    "        MemoryUnit.setActionValue(action,value)\n",
    "\n",
    "    def addMemory(self,state,action,value):#called in the case that when a state first insert into memory\n",
    "        newMemoryUnit=MemoryUnit(state)\n",
    "        newMemoryUnit.actionValues[action]=value\n",
    "        self.longtermmemory.append(newMemoryUnit)\n",
    "\n",
    "    def stateEq(self,state1,state2):#state is a numpy array\n",
    "        return (state1==state2).all()\n",
    "\n",
    "    def getChoiceAction(self):#get a choose action from a given state\n",
    "        memory=False\n",
    "        memory=self.searchMemory(self.environment.game.grid)\n",
    "        if memory==False:\n",
    "            available_actions=self.environment.game.available_actions.tolist()\n",
    "            return available_actions[random.randint(0,len(available_actions)-1)]\n",
    "        else:\n",
    "            if random.random()>self.ε:\n",
    "                return memory.getmaxAction(self.environment.game.available_actions.tolist())#########\n",
    "            else:\n",
    "                available_actions=self.environment.game.available_actions.tolist()\n",
    "                return available_actions[random.randint(0,len(available_actions)-1)]\n",
    "    \n",
    "    def getNowstate(self):#get the state of the env now\n",
    "        return self.environment.game.grid\n",
    "    \n",
    "    def makeAction(self):\n",
    "        action=self.getChoiceAction()\n",
    "        self.workmemory={'state':copy.deepcopy(self.getNowstate()),'action':copy.deepcopy(action)}#let the workmemory be the state-action pair just have made\n",
    "        self.environment.game.act(action)\n",
    "        self.environment.game.change_turn()\n",
    "\n",
    "    def getReward(self):#only deal with the case that the agent action is not a winning move or draw move(that means the enemy have just made a action)\n",
    "        R=0\n",
    "        memoryReward=0#init two part of reward\n",
    "        '''\n",
    "        if self.environment.game.was_winning_move('o'):\n",
    "            R=-1\n",
    "            memoryReward=0\n",
    "        elif self.environment.game.grid_is_full():\n",
    "            R=0\n",
    "            memoryReward=0\n",
    "        else:\n",
    "        '''\n",
    "        R=0\n",
    "        nowstate=self.getNowstate()\n",
    "        findMemoryUnit=self.searchMemory(nowstate)\n",
    "        if findMemoryUnit==False:\n",
    "            memoryReward=0\n",
    "        else:\n",
    "            memoryReward=findMemoryUnit.getmaxActionValue()\n",
    "        return {'R':R,'memoryreward':memoryReward}\n",
    "\n",
    "\n",
    "    def learn(self,reward):#learn by init and add memoryUnit into memory or edit existed memoryUnit\n",
    "        R=reward['R']\n",
    "        memoreward=reward['memoryreward']\n",
    "        prevoiusState=self.workmemory['state']\n",
    "        prevoiusAction=self.workmemory['action']\n",
    "        findMemoryUnit=self.searchMemory(prevoiusState)\n",
    "        if findMemoryUnit==False:\n",
    "            initvalue=self.α*(R+self.γ*memoreward)\n",
    "            self.addMemory(prevoiusState,prevoiusAction,initvalue)\n",
    "        else:\n",
    "            oldvalue=findMemoryUnit.actionValues[prevoiusAction]\n",
    "            newvalue=(1-self.α)*oldvalue+self.α*(R+self.γ*memoreward)\n",
    "            self.editMemory(findMemoryUnit,prevoiusAction,newvalue)\n",
    "\n",
    "    def interact(self):#a interact include decide a action,make the action,the action of env(enemy),and learn from reward.\n",
    "        if self.environment.game.grid_is_full():#the action that enemy draw\n",
    "            reward={'R':0,'memoryreward':0}\n",
    "            self.learn(reward)\n",
    "            self.environment.game.reset(first_player='o')\n",
    "            self.environment.makeEnvAction(True)\n",
    "\n",
    "        self.makeAction()\n",
    "\n",
    "        if self.environment.game.was_winning_move('x'):\n",
    "            reward={'R':1,'memoryreward':0}\n",
    "            self.learn(reward)\n",
    "            self.environment.makeEnvAction(True)\n",
    "        elif self.environment.game.grid_is_full():#means a draw\n",
    "            reward={'R':0,'memoryreward':0}\n",
    "            self.learn(reward)\n",
    "            self.environment.makeEnvAction(True)\n",
    "        else:\n",
    "            self.environment.makeEnvAction(False)\n",
    "            if self.environment.game.grid_is_full():#the action that enemy draw\n",
    "                reward={'R':0,'memoryreward':0}\n",
    "                self.learn(reward)\n",
    "                self.environment.game.reset(first_player='o')\n",
    "                self.environment.makeEnvAction(True)\n",
    "            elif self.environment.game.was_winning_move('o'):\n",
    "                reward={'R':-1,'memoryreward':0}\n",
    "                self.learn(reward)\n",
    "                self.environment.game.reset(first_player='o')\n",
    "                self.environment.makeEnvAction(True)\n",
    "            else:\n",
    "                reward=self.getReward()\n",
    "                self.learn(reward)\n",
    "\n",
    "    def play_a_game(self):#return the result of the game(win:1,lose:-1,draw:0)\n",
    "        def judgeGameState():\n",
    "            if self.environment.game.player_at_turn=='o' and self.environment.game.was_winning_move('x'):\n",
    "                return win#player_at_turn is after a change made by previousActionner,so'o'means previous action is made my agent\n",
    "            elif self.environment.game.player_at_turn=='x' and  self.environment.game.was_winning_move('o'):\n",
    "                return lose\n",
    "            elif self.environment.game.grid_is_full():\n",
    "                return draw\n",
    "            else:\n",
    "                return running\n",
    "\n",
    "        self.environment.game.reset(first_player='o')\n",
    "        running=10\n",
    "        win=1\n",
    "        lose=-1\n",
    "        draw=0#four kinds of game states\n",
    "        step=0\n",
    "        while(True):\n",
    "            if step==0:\n",
    "                isfirst=True\n",
    "            else:\n",
    "                isfirst=False\n",
    "            self.environment.makeEnvAction(isfirst)\n",
    "            newgamestate=judgeGameState()\n",
    "\n",
    "            if newgamestate==win:\n",
    "                return win\n",
    "            elif newgamestate==lose:\n",
    "                return lose\n",
    "            elif newgamestate==draw:\n",
    "                return draw\n",
    "\n",
    "            self.makeAction()\n",
    "            newgamestate=judgeGameState()\n",
    "  \n",
    "            if newgamestate==win:\n",
    "                return win\n",
    "            elif newgamestate==lose:\n",
    "                return lose\n",
    "            elif newgamestate==draw:\n",
    "                return draw\n",
    "            step+=1\n",
    "        \n",
    "    def trainAndShow(self,k=K,n=20,m=10):\n",
    "        def play_m_games_and_sum_up():\n",
    "            summary=0\n",
    "            newAgent=Agent()\n",
    "            newAgent.longtermmemory=copy.deepcopy(self.longtermmemory)\n",
    "            for i in range(m):\n",
    "                summary=summary+newAgent.play_a_game()\n",
    "            return summary\n",
    "\n",
    "        self.environment.game.reset(first_player='o')\n",
    "        self.environment.makeEnvAction(True)\n",
    "        for i in range(k):\n",
    "            for j in range(n):\n",
    "                self.interact()\n",
    "            res=play_m_games_and_sum_up()\n",
    " #           print(res)\n",
    "            self.resultlist.append(res)\n",
    "\n",
    "\n",
    "def draw(resultlist,randlist):\n",
    "    x=[i*20 for i in (range(len(resultlist)))]\n",
    "    plt.figure()  \n",
    "    plt.plot(x,resultlist,'r', label='Q-learning')\n",
    "    plt.plot(x,randlist,'b',label='Random Agent')\n",
    "    plt.xlabel(\"interaction steps\")  \n",
    "    plt.ylabel(\"value\")  \n",
    "    plt.show() \n",
    " \n",
    "agentnum=1\n",
    "\n",
    "agentlsit=[]\n",
    "randomagentlist=[]\n",
    "\n",
    "for i in range(agentnum):\n",
    "    agent=Agent()#####\n",
    "    agentlsit.append(agent)\n",
    "for agent in agentlsit:\n",
    "    agent.trainAndShow(k=250)\n",
    "\n",
    "for i in range(agentnum):\n",
    "    agent=ra.randomAgent()#####\n",
    "    randomagentlist.append(agent)\n",
    "for agent in randomagentlist:\n",
    "    agent.trainAndShow(k=250)\n",
    "\n",
    "def sumagentlistresult():\n",
    "    retlist=[]\n",
    "    for i in range(250):\n",
    "        retlist.append(0)\n",
    "    for agent in agentlsit:\n",
    "        retlist = map(lambda x, y: x + y, retlist,agent.resultlist)\n",
    "    retlist=list(map(lambda x:x/agentnum,retlist))\n",
    "    return retlist\n",
    "\n",
    "def sumrandomagentlistresult():\n",
    "    retlist=[]\n",
    "    for i in range(250):\n",
    "        retlist.append(0)\n",
    "    for agent in randomagentlist:\n",
    "        retlist = map(lambda x, y: x + y, retlist,agent.resultlist)\n",
    "    retlist=list(map(lambda x:x/agentnum,retlist))\n",
    "    return retlist\n",
    "\n",
    "retlist=sumagentlistresult()\n",
    "randomlist=sumrandomagentlistresult()\n",
    "draw(retlist,randomlist)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2A\n",
    " \n",
    "Using the minimax policy you computed, answer the following question: The first player (Player 1) drops his/her first disk into column 2 (counting from the left). Consider the resulting state, shown in the following code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = connect.Connect(verbose=False)\n",
    "env.reset(first_player='o')\n",
    "env.act(action=1)\n",
    "print(env.grid[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Minimax Value of this state for Player 2? That is, assuming an optimal opponent, does the Minimax Policy expect to win the game (value = 1), lose the game (value = –1), or end the game in a draw (value = 0)? Please state your answer as a number.    \n",
    "\n",
    "* The code cell below should compute this value and assign it to a variable called `state_value`.\n",
    "* Count the number of branches of the game tree that were examined and assign this number to a variable called `num_branches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7b00948d6cc98e71a2f9467263067bc0",
     "grade": false,
     "grade_id": "cell-9e3a2a1bebc09565",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' 'o' ' ' ' ' ' ']]\n",
      "-1\n",
      "4855\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import connect\n",
    "import numpy as np \n",
    "import copy\n",
    "import random\n",
    "import RandomAgent as ra\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "K=200\n",
    "env = connect.Connect(verbose=False)\n",
    "env.reset(first_player='o')\n",
    "env.act(action=1)\n",
    "print(env.grid[::-1])\n",
    "env.change_turn()\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.game=connect.Connect(verbose=False)\n",
    "        self.game.reset(first_player='o')\n",
    "    \n",
    "    def choiceRandAction(self):\n",
    "        available=self.game.available_actions\n",
    "        return available[random.randint(0,len(available.tolist())-1)]\n",
    "    \n",
    "    def makeRandAction(self):\n",
    "        action=self.choiceRandAction()\n",
    "        self.game.act(action)\n",
    "\n",
    "    def makeEnvAction(self,isfirst):\n",
    "        if isfirst:\n",
    "            self.game.reset(first_player='o')\n",
    "            self.makeRandAction()\n",
    "            self.game.change_turn()\n",
    "        else:\n",
    "            if self.game.was_winning_move('x'):\n",
    "                self.game.reset(first_player='o')\n",
    "                self.makeRandAction()\n",
    "                self.game.change_turn()\n",
    "            elif self.game.grid_is_full():#means a draw\n",
    "                self.game.reset(first_player='o')\n",
    "                self.makeRandAction()\n",
    "                self.game.change_turn()\n",
    "            else:\n",
    "                self.makeRandAction()\n",
    "                self.game.change_turn()\n",
    "\n",
    "\n",
    "class minimaxAgent:\n",
    "    def __init__(self,environment=Environment()):\n",
    "        self.environment=environment\n",
    "        self.resultlist=[]\n",
    "        self.count=0\n",
    "    \n",
    "    def AlphaBetaMinimax(self,recenv,player,alpha,beta):\n",
    "        previous_player=recenv.game.other_player[player]\n",
    "        if recenv.game.was_winning_move(previous_player):\n",
    "            if previous_player=='o':\n",
    "                self.count+=1\n",
    "                return {'R':-1,'A':'over'}\n",
    "            elif previous_player=='x':\n",
    "                self.count+=1\n",
    "                return {'R':1,'A':'over'}\n",
    "        elif recenv.game.grid_is_full():\n",
    "            self.count+=1\n",
    "            return {'R':0,'A':'over'}\n",
    "        else:\n",
    "            if player=='x':#looking for max\n",
    "                availableActions=recenv.game.available_actions.copy()\n",
    "                retaction='init'\n",
    "                for action in availableActions:\n",
    "                    newenv=copy.deepcopy(recenv)\n",
    "                    newenv.game.act(action)\n",
    "                    newenv.game.change_turn()\n",
    "                    actionvalue=self.AlphaBetaMinimax(newenv,'o',alpha,beta)['R']\n",
    "                    if actionvalue>alpha:\n",
    "                        alpha=actionvalue\n",
    "                        retaction=action\n",
    "                    if alpha>=beta:\n",
    "                        break\n",
    "                return {'R':alpha,'A':retaction}\n",
    "            if player=='o':#looking for min\n",
    "                availableActions=recenv.game.available_actions.copy()\n",
    "                retaction='init'\n",
    "                for action in availableActions:\n",
    "                    newenv=copy.deepcopy(recenv)\n",
    "                    newenv.game.act(action)\n",
    "                    newenv.game.change_turn()\n",
    "                    actionvalue=self.AlphaBetaMinimax(newenv,'x',alpha,beta)['R']\n",
    "                    if actionvalue<beta:\n",
    "                        beta=actionvalue\n",
    "                        retaction=action\n",
    "                    if beta<=alpha:\n",
    "                        break\n",
    "                return {'R':beta,'A':retaction}\n",
    "\n",
    "    def getChoiceAction(self):\n",
    "        return self.AlphaBetaMinimax(self.environment,'x',-10,+10)['A']\n",
    "\n",
    "    def makeAction(self):\n",
    "        action=self.getChoiceAction()\n",
    "        self.environment.game.act(action)\n",
    "        self.environment.game.change_turn()\n",
    "\n",
    "\n",
    "    def play_a_game(self):#return the result of the game(win:1,lose:-1,draw:0)#############测不出胜负全平！！！！\n",
    "        def judgeGameState():\n",
    "            if self.environment.game.player_at_turn=='o' and self.environment.game.was_winning_move('x'):\n",
    "                return win#player_at_turn is after a change made by previousActionner,so'o'means previous action is made my agent\n",
    "            elif self.environment.game.player_at_turn=='x' and  self.environment.game.was_winning_move('o'):\n",
    "                return lose\n",
    "            elif self.environment.game.grid_is_full():\n",
    "                return draw\n",
    "            else:\n",
    "                return running\n",
    "\n",
    "        self.environment.game.reset(first_player='o')\n",
    "        running=10\n",
    "        win=1\n",
    "        lose=-1\n",
    "        draw=0#four kinds of game states\n",
    "        step=0\n",
    "        while(True):\n",
    "            if step==0:\n",
    "                isfirst=True\n",
    "            else:\n",
    "                isfirst=False\n",
    "            self.environment.makeEnvAction(isfirst)\n",
    "            newgamestate=judgeGameState()\n",
    "\n",
    "            if newgamestate==win:\n",
    "                return win\n",
    "            elif newgamestate==lose:\n",
    "                return lose\n",
    "            elif newgamestate==draw:\n",
    "                return draw\n",
    "\n",
    "            self.makeAction()\n",
    "            newgamestate=judgeGameState()\n",
    "  \n",
    "            if newgamestate==win:\n",
    "                return win\n",
    "            elif newgamestate==lose:\n",
    "                return lose\n",
    "            elif newgamestate==draw:\n",
    "                return draw\n",
    "            step+=1\n",
    "    \n",
    "    def trainAndShow(self,k=K,n=20,m=10):\n",
    "        def play_m_games_and_sum_up():\n",
    "            summary=0\n",
    "            newminimaxAgent=minimaxAgent()\n",
    "            for i in range(m):\n",
    "                summary=summary+newminimaxAgent.play_a_game()\n",
    "            return summary\n",
    "\n",
    "        self.environment.game.reset(first_player='o')\n",
    "        self.environment.makeEnvAction(True)\n",
    "        for i in range(k):\n",
    "            res=play_m_games_and_sum_up()\n",
    "            print(res)\n",
    "            self.resultlist.append(res)\n",
    "\n",
    "\n",
    "def draw(resultlist,randlist):\n",
    "    x=list(range(len(resultlist))) \n",
    "    plt.figure()  \n",
    "    plt.plot(x,resultlist,'r', label='Q-learning')\n",
    "    plt.plot(x,randlist,'b',label='Random Agent')\n",
    "    plt.xlabel(\"k\")  \n",
    "    plt.ylabel(\"value\")  \n",
    "    plt.savefig(\"returnminimax.jpg\")  \n",
    "a=minimaxAgent()\n",
    "\n",
    "\n",
    "e=Environment()\n",
    "e.game=env          \n",
    "state_value = a.AlphaBetaMinimax(e,'x',-10,+10)['R']\n",
    "num_branches = a.count\n",
    "print(state_value)\n",
    "print(num_branches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d9e96fd7c7ed9d9081eb564773f199c",
     "grade": true,
     "grade_id": "cell-c31d5222d21dd1d5",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograded test cell. Do not delete or change, otherwise you will get \n",
    "# no marks for this part of the assignment. Please make sure that this cell has \n",
    "# access to the variables state_value and num_branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2B\n",
    "Plot a learning curve similar to the one in Part 1, comparing your Q-learning algorithm, random play, and Minimax play. Assume as before that the opponent always plays first and uses a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f6ed321958b16409ec1935b0fef7aa0d",
     "grade": true,
     "grade_id": "cell-a1d1652414bc7967",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' 'o' ' ' ' ' ' ']]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fdd4fdc56038>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[0magentlsit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0magentlsit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainAndShow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magentnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-fdd4fdc56038>\u001b[0m in \u001b[0;36mtrainAndShow\u001b[1;34m(self, k, n, m)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakeEnvAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplay_m_games_and_sum_up\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m             \u001b[1;31m#print(res)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresultlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-fdd4fdc56038>\u001b[0m in \u001b[0;36mplay_m_games_and_sum_up\u001b[1;34m()\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mnewminimaxAgent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminimaxAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                 \u001b[0msummary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnewminimaxAgent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_a_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-fdd4fdc56038>\u001b[0m in \u001b[0;36mplay_a_game\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakeAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[0mnewgamestate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjudgeGameState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-fdd4fdc56038>\u001b[0m in \u001b[0;36mmakeAction\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmakeAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[0maction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetChoiceAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchange_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-fdd4fdc56038>\u001b[0m in \u001b[0;36mgetChoiceAction\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetChoiceAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAlphaBetaMinimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmakeAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-fdd4fdc56038>\u001b[0m in \u001b[0;36mAlphaBetaMinimax\u001b[1;34m(self, recenv, player, alpha, beta)\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchange_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                     \u001b[0mactionvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAlphaBetaMinimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewenv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'R'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mactionvalue\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                         \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactionvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-fdd4fdc56038>\u001b[0m in \u001b[0;36mAlphaBetaMinimax\u001b[1;34m(self, recenv, player, alpha, beta)\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchange_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[0mactionvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAlphaBetaMinimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewenv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'R'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mactionvalue\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                         \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactionvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-fdd4fdc56038>\u001b[0m in \u001b[0;36mAlphaBetaMinimax\u001b[1;34m(self, recenv, player, alpha, beta)\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchange_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                     \u001b[0mactionvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAlphaBetaMinimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewenv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'R'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mactionvalue\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                         \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactionvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-fdd4fdc56038>\u001b[0m in \u001b[0;36mAlphaBetaMinimax\u001b[1;34m(self, recenv, player, alpha, beta)\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchange_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[0mactionvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAlphaBetaMinimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewenv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'R'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mactionvalue\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                         \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactionvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-fdd4fdc56038>\u001b[0m in \u001b[0;36mAlphaBetaMinimax\u001b[1;34m(self, recenv, player, alpha, beta)\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchange_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                     \u001b[0mactionvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAlphaBetaMinimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewenv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'R'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mactionvalue\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                         \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactionvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-fdd4fdc56038>\u001b[0m in \u001b[0;36mAlphaBetaMinimax\u001b[1;34m(self, recenv, player, alpha, beta)\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchange_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[0mactionvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAlphaBetaMinimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewenv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'R'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mactionvalue\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                         \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactionvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-fdd4fdc56038>\u001b[0m in \u001b[0;36mAlphaBetaMinimax\u001b[1;34m(self, recenv, player, alpha, beta)\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchange_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                     \u001b[0mactionvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAlphaBetaMinimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewenv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'R'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mactionvalue\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                         \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactionvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-fdd4fdc56038>\u001b[0m in \u001b[0;36mAlphaBetaMinimax\u001b[1;34m(self, recenv, player, alpha, beta)\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchange_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[0mactionvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAlphaBetaMinimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewenv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'R'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mactionvalue\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                         \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactionvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-fdd4fdc56038>\u001b[0m in \u001b[0;36mAlphaBetaMinimax\u001b[1;34m(self, recenv, player, alpha, beta)\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[0mretaction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'init'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mavailableActions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecenv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                     \u001b[0mnewenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchange_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# If is its own copy, don't memoize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__setstate__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# If is its own copy, don't memoize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__setstate__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__deepcopy__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[0mreductor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import connect\n",
    "import numpy as np \n",
    "import copy\n",
    "import random\n",
    "import RandomAgent as ra\n",
    "import Qlearning \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "\n",
    "K=200\n",
    "env = connect.Connect(verbose=False)\n",
    "env.reset(first_player='o')\n",
    "env.act(action=1)\n",
    "print(env.grid[::-1])\n",
    "env.change_turn()\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.game=connect.Connect(verbose=False)\n",
    "        self.game.reset(first_player='o')\n",
    "    \n",
    "    def choiceRandAction(self):\n",
    "        available=self.game.available_actions\n",
    "        return available[random.randint(0,len(available.tolist())-1)]\n",
    "    \n",
    "    def makeRandAction(self):\n",
    "        action=self.choiceRandAction()\n",
    "        self.game.act(action)\n",
    "\n",
    "    def makeEnvAction(self,isfirst):\n",
    "        if isfirst:\n",
    "            self.game.reset(first_player='o')\n",
    "            self.makeRandAction()\n",
    "            self.game.change_turn()\n",
    "        else:\n",
    "            if self.game.was_winning_move('x'):\n",
    "                self.game.reset(first_player='o')\n",
    "                self.makeRandAction()\n",
    "                self.game.change_turn()\n",
    "            elif self.game.grid_is_full():#means a draw\n",
    "                self.game.reset(first_player='o')\n",
    "                self.makeRandAction()\n",
    "                self.game.change_turn()\n",
    "            else:\n",
    "                self.makeRandAction()\n",
    "                self.game.change_turn()\n",
    "\n",
    "\n",
    "class minimaxAgent:\n",
    "    def __init__(self,environment=Environment()):\n",
    "        self.environment=environment\n",
    "        self.resultlist=[]\n",
    "        self.count=0\n",
    "    \n",
    "    def AlphaBetaMinimax(self,recenv,player,alpha,beta):\n",
    "        previous_player=recenv.game.other_player[player]\n",
    "        if recenv.game.was_winning_move(previous_player):\n",
    "            if previous_player=='o':\n",
    "                self.count+=1\n",
    "                return {'R':-1,'A':'over'}\n",
    "            elif previous_player=='x':\n",
    "                self.count+=1\n",
    "                return {'R':1,'A':'over'}\n",
    "        elif recenv.game.grid_is_full():\n",
    "            self.count+=1\n",
    "            return {'R':0,'A':'over'}\n",
    "        else:\n",
    "            if player=='x':#looking for max\n",
    "                availableActions=recenv.game.available_actions.copy()\n",
    "                retaction='init'\n",
    "                for action in availableActions:\n",
    "                    newenv=copy.deepcopy(recenv)\n",
    "                    newenv.game.act(action)\n",
    "                    newenv.game.change_turn()\n",
    "                    actionvalue=self.AlphaBetaMinimax(newenv,'o',alpha,beta)['R']\n",
    "                    if actionvalue>alpha:\n",
    "                        alpha=actionvalue\n",
    "                        retaction=action\n",
    "                    if alpha>=beta:\n",
    "                        break\n",
    "                return {'R':alpha,'A':retaction}\n",
    "            if player=='o':#looking for min\n",
    "                availableActions=recenv.game.available_actions.copy()\n",
    "                retaction='init'\n",
    "                for action in availableActions:\n",
    "                    newenv=copy.deepcopy(recenv)\n",
    "                    newenv.game.act(action)\n",
    "                    newenv.game.change_turn()\n",
    "                    actionvalue=self.AlphaBetaMinimax(newenv,'x',alpha,beta)['R']\n",
    "                    if actionvalue<beta:\n",
    "                        beta=actionvalue\n",
    "                        retaction=action\n",
    "                    if beta<=alpha:\n",
    "                        break\n",
    "                return {'R':beta,'A':retaction}\n",
    "\n",
    "    def getChoiceAction(self):\n",
    "        return self.AlphaBetaMinimax(self.environment,'x',-10,+10)['A']\n",
    "\n",
    "    def makeAction(self):\n",
    "        action=self.getChoiceAction()\n",
    "        self.environment.game.act(action)\n",
    "        self.environment.game.change_turn()\n",
    "\n",
    "\n",
    "    def play_a_game(self):#return the result of the game(win:1,lose:-1,draw:0)#############测不出胜负全平！！！！\n",
    "        def judgeGameState():\n",
    "            if self.environment.game.player_at_turn=='o' and self.environment.game.was_winning_move('x'):\n",
    "                return win#player_at_turn is after a change made by previousActionner,so'o'means previous action is made my agent\n",
    "            elif self.environment.game.player_at_turn=='x' and  self.environment.game.was_winning_move('o'):\n",
    "                return lose\n",
    "            elif self.environment.game.grid_is_full():\n",
    "                return draw\n",
    "            else:\n",
    "                return running\n",
    "\n",
    "        self.environment.game.reset(first_player='o')\n",
    "        running=10\n",
    "        win=1\n",
    "        lose=-1\n",
    "        draw=0#four kinds of game states\n",
    "        step=0\n",
    "        while(True):\n",
    "            if step==0:\n",
    "                isfirst=True\n",
    "            else:\n",
    "                isfirst=False\n",
    "            self.environment.makeEnvAction(isfirst)\n",
    "            newgamestate=judgeGameState()\n",
    "\n",
    "            if newgamestate==win:\n",
    "                return win\n",
    "            elif newgamestate==lose:\n",
    "                return lose\n",
    "            elif newgamestate==draw:\n",
    "                return draw\n",
    "\n",
    "            self.makeAction()\n",
    "            newgamestate=judgeGameState()\n",
    "  \n",
    "            if newgamestate==win:\n",
    "                return win\n",
    "            elif newgamestate==lose:\n",
    "                return lose\n",
    "            elif newgamestate==draw:\n",
    "                return draw\n",
    "            step+=1\n",
    "    \n",
    "    def trainAndShow(self,k=K,n=20,m=10):\n",
    "        def play_m_games_and_sum_up():\n",
    "            summary=0\n",
    "            newminimaxAgent=minimaxAgent()\n",
    "            for i in range(m):\n",
    "                summary=summary+newminimaxAgent.play_a_game()\n",
    "            return summary\n",
    "\n",
    "        self.environment.game.reset(first_player='o')\n",
    "        self.environment.makeEnvAction(True)\n",
    "        for i in range(k):\n",
    "            res=play_m_games_and_sum_up()\n",
    "            print(res)\n",
    "            self.resultlist.append(res)\n",
    "\n",
    "\n",
    "def draw(resultlist,Qlist,randlist):\n",
    "    x=[i*20 for i in (range(len(resultlist)))]\n",
    "    plt.figure()  \n",
    "    plt.plot(x,resultlist,'r', label='minimax')\n",
    "    plt.plot(x,Qlist,'k',label='Q-learing')\n",
    "    plt.plot(x,randlist,'b',label='Random Agent')\n",
    "    plt.xlabel(\"interaction steps\")  \n",
    "    plt.ylabel(\"value\")  \n",
    "    plt.savefig(\"returnminimax.jpg\")  \n",
    "\n",
    "\n",
    "'''\n",
    "e=Environment()\n",
    "e.game=env\n",
    "a=minimaxAgent()\n",
    "print(a.AlphaBetaMinimax(e,'x',-10,+10))\n",
    "print(a.count)\n",
    "'''         \n",
    "agentnum=1\n",
    "\n",
    "minimaxagentlsit=[]\n",
    "randomagentlist=[]\n",
    "Qagentlist=[]\n",
    "\n",
    "for i in range(agentnum):\n",
    "    agent=minimaxAgent()#####\n",
    "    minimaxagentlsit.append(agent)\n",
    "for agent in minimaxagentlsit:\n",
    "    agent.trainAndShow(k=400)\n",
    "\n",
    "for i in range(agentnum):\n",
    "    agent=ra.randomAgent()#####\n",
    "    randomagentlist.append(agent)\n",
    "for agent in randomagentlist:\n",
    "    agent.trainAndShow(k=400)\n",
    "\n",
    "for i in range(agentnum):\n",
    "    agent=Qlearning.Agent()\n",
    "    Qagentlist.append(agent)\n",
    "for agent in Qagentlist:\n",
    "    agent.trainAndShow(k=400)\n",
    "def sumagentlistresult():\n",
    "    retlist=[]\n",
    "    for i in range(400):\n",
    "        retlist.append(0)\n",
    "    for agent in minimaxagentlsit:\n",
    "        retlist = map(lambda x, y: x + y, retlist,agent.resultlist)\n",
    "    retlist=list(map(lambda x:x/agentnum,retlist))\n",
    "    return retlist\n",
    "\n",
    "def sumrandomagentlistresult():\n",
    "    retlist=[]\n",
    "    for i in range(400):\n",
    "        retlist.append(0)\n",
    "    for agent in randomagentlist:\n",
    "        retlist = map(lambda x, y: x + y, retlist,agent.resultlist)\n",
    "    retlist=list(map(lambda x:x/agentnum,retlist))\n",
    "    return retlist\n",
    "\n",
    "def sumQagentlistresult():\n",
    "    retlist=[]\n",
    "    for i in range(400):\n",
    "        retlist.append(0)\n",
    "    for agent in Qagentlist:\n",
    "        retlist=map(lambda x,y:x+y,retlist,agent.resultlist)\n",
    "    retlist=list(map(lambda x:x/agentnum,retlist))\n",
    "    return retlist\n",
    "retlist=sumagentlistresult()\n",
    "randomlist=sumrandomagentlistresult()\n",
    "Qlist=sumQagentlistresult()\n",
    "draw(retlist,Qlist,randomlist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "614920eb9ba438cf5626dd59edc30252",
     "grade": true,
     "grade_id": "cell-1ea89dfffb81a041",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Explain your findings in 3 or fewer sentences. Which policy is better? Why?\n",
    "\n",
    "<img src=\"images/part2image.jpg\" style=\"width: 600px;\"/>\n",
    "the minimax policy is stable,but minimax search will spend a lot of time.Q-learning will cost lot if time to train,but at last we would get a policy that could could be used very \n",
    "efficiency.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Using your algorithm, compute the value of the game for your player (recall: your player goes second against a random opponent). The code cell below should compute this value and assign it to a variable called `optimal_policy_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d7e4093f906a825959634f4ee6e6845f",
     "grade": false,
     "grade_id": "cell-a28f4f38b5cc6619",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' 'o' ' ' ' ' ' ']]\n",
      "4\n",
      "0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "import connect\n",
    "import numpy as np \n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "import RandomAgent as ra\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "K=200\n",
    "env = connect.Connect(verbose=False)\n",
    "env.reset(first_player='o')\n",
    "env.act(action=1)\n",
    "print(env.grid[::-1])\n",
    "env.change_turn()\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.game=connect.Connect(verbose=False)\n",
    "        self.game.reset(first_player='o')\n",
    "    \n",
    "    def choiceRandAction(self):\n",
    "        available=self.game.available_actions\n",
    "        return available[random.randint(0,len(available.tolist())-1)]\n",
    "    \n",
    "    def makeRandAction(self):\n",
    "        action=self.choiceRandAction()\n",
    "        self.game.act(action)\n",
    "\n",
    "    def makeEnvAction(self,isfirst):\n",
    "        if isfirst:\n",
    "            self.game.reset(first_player='o')\n",
    "            self.makeRandAction()\n",
    "            self.game.change_turn()\n",
    "        else:\n",
    "            if self.game.was_winning_move('x'):\n",
    "                self.game.reset(first_player='o')\n",
    "                self.makeRandAction()\n",
    "                self.game.change_turn()\n",
    "            elif self.game.grid_is_full():#means a draw\n",
    "                self.game.reset(first_player='o')\n",
    "                self.makeRandAction()\n",
    "                self.game.change_turn()\n",
    "            else:\n",
    "                self.makeRandAction()\n",
    "                self.game.change_turn()\n",
    "            \n",
    "\n",
    "class MCTNode:\n",
    "    def __init__(self,state,player):\n",
    "        self.state=state\n",
    "        self.player=player\n",
    "        self.availableActions=state.available_actions\n",
    "        self.father=None\n",
    "        self.childdict={}# key为动作，值为子节点\n",
    "        self.count=0\n",
    "        self.playerwins={'o':0,'x':0}\n",
    "        self.C=0.05\n",
    "    \n",
    "    def getmaxaction(self):\n",
    "        maxval=0\n",
    "        retaction=None\n",
    "        for key in self.childdict:\n",
    "            child=self.childdict[key]\n",
    "            #val=child.playerwins[self.player]/child.count\n",
    "            val=child.count\n",
    "            if val>maxval:\n",
    "                retaction=key\n",
    "        return retaction\n",
    "    \n",
    "    def getstatevalue(self):\n",
    "        return self.playerwins[self.player]/self.count\n",
    "\n",
    "    def incwin(self,player):\n",
    "        self.playerwins[player]=self.playerwins[player]+1\n",
    "\n",
    "\n",
    "    def backpropagation(self,winner):\n",
    "        if self.father==None:\n",
    "            if winner=='draw':\n",
    "                self.count+=1\n",
    "            else:\n",
    "                self.count+=1\n",
    "                self.incwin(self.player)\n",
    "        else:\n",
    "            if winner=='draw':\n",
    "                self.count+=1\n",
    "                self.father.backpropagation(winner)\n",
    "            else:\n",
    "                self.count+=1\n",
    "                self.incwin(self.player)\n",
    "                self.father.backpropagation(winner)\n",
    "    \n",
    "    def getUCB(self,player):\n",
    "        if self.count==0:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.playerwins[player]/self.count+self.C*math.sqrt(math.log(self.father.count)/self.count)\n",
    "    \n",
    "    def select(self,player):\n",
    "        maxval=-1\n",
    "        thenode=None\n",
    "        retaction=None\n",
    "        if self.state.grid_is_full():\n",
    "            return {'node':self,'action':'draw'}\n",
    "        elif self.state.was_winning_move(self.state.other_player[self.state.player_at_turn]):\n",
    "            return {'node':self,'action':self.state.other_player[self.state.player_at_turn]}\n",
    "        for action in self.availableActions:\n",
    "            if action in self.childdict:\n",
    "                child=self.childdict[action]\n",
    "                childUCB=child.getUCB(player)#################\n",
    "                if maxval<=childUCB:\n",
    "                    thenode=child\n",
    "                    retaction=action\n",
    "                    maxval=childUCB\n",
    "            else:\n",
    "                thenode=self\n",
    "                retaction=action\n",
    "                return {'node':thenode,'action':retaction}\n",
    "        return thenode.select(player)\n",
    "        \n",
    "    \n",
    "    def expansion(self,action):#return the child that new expan\n",
    "        newstate=copy.deepcopy(self.state)\n",
    "        newstate.act(action)\n",
    "        newstate.change_turn()\n",
    "        newchild=MCTNode(newstate,newstate.player_at_turn)\n",
    "        newchild.father=self\n",
    "        self.childdict[action]=newchild\n",
    "        return newchild\n",
    "    \n",
    "    def simulation(self):\n",
    "        newenv=copy.deepcopy(self.state)\n",
    "        if newenv.was_winning_move(newenv.other_player[self.player]):\n",
    "            return newenv.other_player[self.player]\n",
    "        elif newenv.grid_is_full():\n",
    "            return 'draw'\n",
    "        else:\n",
    "            while(True):\n",
    "                available=newenv.available_actions\n",
    "                action=available[random.randint(0,len(available.tolist())-1)]\n",
    "                newenv.act(action)\n",
    "                if newenv.was_winning_move(newenv.player_at_turn):\n",
    "                    return newenv.player_at_turn\n",
    "                elif newenv.grid_is_full():\n",
    "                    return 'draw'\n",
    "                newenv.change_turn()\n",
    "            \n",
    "\n",
    "\n",
    "class MCTSAgent:\n",
    "    def __init__(self,environment=Environment()):\n",
    "        self.environment=environment\n",
    "        self.resultlist=[]\n",
    "    \n",
    "    def MCTS(self,env,player):\n",
    "        root=MCTNode(copy.deepcopy(env),player)#the state of tree node is a connect object\n",
    "        maxturn=300\n",
    "        count=0\n",
    "        while(count<maxturn):\n",
    "            selectresult=root.select(player)\n",
    "            if selectresult['action']=='draw':\n",
    "                selectresult['node'].backpropagation('draw')\n",
    "                count+=1\n",
    "                continue\n",
    "            elif selectresult['action']=='o':\n",
    "                selectresult['node'].backpropagation('o')\n",
    "                count+=1\n",
    "                continue\n",
    "            elif selectresult['action']=='x':\n",
    "                selectresult['node'].backpropagation('x')\n",
    "                count+=1\n",
    "                continue\n",
    "            nodetoexpan=selectresult['node']\n",
    "            nodetosimulate=nodetoexpan.expansion(selectresult['action'])\n",
    "            simulateresult=nodetosimulate.simulation()\n",
    "            nodetosimulate.backpropagation(simulateresult)\n",
    "            count+=1\n",
    "        return {'action':root.getmaxaction(),'statevalue':root.getstatevalue()}\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def getChoiceAction(self):\n",
    "        return self.MCTS(self.environment.game,self.environment.game.player_at_turn)['action']\n",
    "\n",
    "    def makeAction(self):\n",
    "        action=self.getChoiceAction()\n",
    "        self.environment.game.act(action)\n",
    "        self.environment.game.change_turn()\n",
    "\n",
    "\n",
    "    def play_a_game(self):#return the result of the game(win:1,lose:-1,draw:0)#############测不出胜负全平！！！！\n",
    "        def judgeGameState():\n",
    "            if self.environment.game.player_at_turn=='o' and self.environment.game.was_winning_move('x'):\n",
    "                return win#player_at_turn is after a change made by previousActionner,so'o'means previous action is made my agent\n",
    "            elif self.environment.game.player_at_turn=='x' and  self.environment.game.was_winning_move('o'):\n",
    "                return lose\n",
    "            elif self.environment.game.grid_is_full():\n",
    "                return draw\n",
    "            else:\n",
    "                return running\n",
    "\n",
    "        self.environment.game.reset(first_player='o')\n",
    "        running=10\n",
    "        win=1\n",
    "        lose=-1\n",
    "        draw=0#four kinds of game states\n",
    "        step=0\n",
    "        while(True):\n",
    "            if step==0:\n",
    "                isfirst=True\n",
    "            else:\n",
    "                isfirst=False\n",
    "            self.environment.makeEnvAction(isfirst)\n",
    "            newgamestate=judgeGameState()\n",
    "\n",
    "            if newgamestate==win:\n",
    "                return win\n",
    "            elif newgamestate==lose:\n",
    "                return lose\n",
    "            elif newgamestate==draw:\n",
    "                return draw\n",
    "\n",
    "            self.makeAction()\n",
    "            newgamestate=judgeGameState()\n",
    "  \n",
    "            if newgamestate==win:\n",
    "                return win\n",
    "            elif newgamestate==lose:\n",
    "                return lose\n",
    "            elif newgamestate==draw:\n",
    "                return draw\n",
    "            step+=1\n",
    "    \n",
    "    def trainAndShow(self,k=K,n=20,m=10):\n",
    "        def play_m_games_and_sum_up():\n",
    "            summary=0\n",
    "            newMCTSAgent=MCTSAgent()\n",
    "            for i in range(m):\n",
    "                summary=summary+newMCTSAgent.play_a_game()\n",
    "            return summary\n",
    "\n",
    "        self.environment.game.reset(first_player='o')\n",
    "        self.environment.makeEnvAction(True)\n",
    "        for i in range(k):\n",
    "            res=play_m_games_and_sum_up()\n",
    "            print(res)\n",
    "            self.resultlist.append(res)\n",
    "\n",
    "\n",
    "def draw(resultlist,randlist):\n",
    "    x=list(range(len(resultlist))) \n",
    "    plt.figure()  \n",
    "    plt.plot(x,resultlist,'r', label='MCTS')\n",
    "    plt.plot(x,randlist,'b',label='Random Agent')\n",
    "    plt.xlabel(\"k\")  \n",
    "    plt.ylabel(\"value\")  \n",
    "    plt.savefig(\"returnMCTS.jpg\")  \n",
    "\n",
    "\n",
    "a=MCTSAgent()\n",
    "a.environment.game=env\n",
    "res=a.MCTS(a.environment.game,a.environment.game.player_at_turn)\n",
    "print(res['action'])\n",
    "print(res['statevalue'])\n",
    "\n",
    "optimal_policy_value = res['statevalue']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b3a945d86dc159320c257d69389cf765",
     "grade": true,
     "grade_id": "cell-1e0341a7a580c299",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograded test cell. Do not delete or change, otherwise you will get \n",
    "# no marks for this part of the assignment. Please make sure that this cell has \n",
    "# access to the variable optimal_policy_value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
